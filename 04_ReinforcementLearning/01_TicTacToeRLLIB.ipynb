{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286af822",
   "metadata": {},
   "source": [
    "## Tic Tac Toc + Reinforcement Learning with SageMaker\n",
    "The goal of this experiment is to have in the end a Tensorflow model that knows how to play Tic Tac Toe. We will be able to create a game or integrate this model to an existing game.  \n",
    "We will use **RayRLlib** + **OpenAI Gym** running as a SageMaker training job. So, let's get our hands dirty and:\n",
    "- create an OpenAI Gym custom environment that represents the board and the rules of the game\n",
    "- create an Heuristics (rule based engine) that will play against the agent to make it learn\n",
    "- train our model using SageMaker\n",
    "- test the model in a Tic Tac Toe match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tictactoe && mkdir -p tictactoe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db466a8f",
   "metadata": {},
   "source": [
    "### 1/4) First we need to create a new OpenAI Gym that represents the board and the game rules\n",
    "This is a multi-agent experiment, so we will create an Env that supports two players simultaneously. To make this work, besides the 9 possible positions in the board we need to create an additional action to represent the player is waiting for the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tictactoe/tictactoe.py\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import ray.rllib as rllib\n",
    "\n",
    "class TicTacToeEnv(rllib.env.MultiAgentEnv, gym.Env):\n",
    "    PLAYER_X=1\n",
    "    PLAYER_O=2\n",
    "    WAIT=9\n",
    "    marker = ['-', 'X', 'O']\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(9 + 1) # 9 valid + wait\n",
    "        self.observation_space = gym.spaces.Box(0,2, [9+1]) # values between 0 and 2\n",
    "        self.reset()\n",
    "\n",
    "    def __defense_detection__(self, board, enemy_id):\n",
    "        \"\"\"Checks if there is an opportunity to defend itself from an attack\"\"\"\n",
    "        board_play=board==enemy_id # agent id\n",
    "        board_mask=board==0 # empty cells\n",
    "        h = np.sum(board_play, axis=1)# horizontal\n",
    "        v = np.sum(board_play, axis=0) # vertical\n",
    "        # diagonals\n",
    "        diagA,diagAMask = np.diagonal(board_play),np.diagonal(board_mask)     \n",
    "        diagB,diagBMask = np.fliplr(board_play).diagonal(),np.fliplr(board_mask).diagonal()\n",
    "        defense_options = []\n",
    "        for idx,row in enumerate(h): # scan rows\n",
    "            if row==2 and np.sum(board_mask[idx])>0:\n",
    "                defense_options.append((idx,np.argmax(board_mask[idx])))\n",
    "        for idx,col in enumerate(v): # scan cols\n",
    "            if col==2 and np.sum(board_mask[:,idx])>0:\n",
    "                defense_options.append((np.argmax(board_mask[:,idx]),idx))\n",
    "        if np.sum(diagA)==2 and np.sum(diagAMask)>0: # scan diagonal A\n",
    "            idx = np.argmax(diagAMask); defense_options.append((idx,idx))\n",
    "        if np.sum(diagB)==2 and np.sum(diagBMask)>0: # scan diagonal B\n",
    "            idx = np.argmax(diagBMask); defense_options.append((idx,2-idx))\n",
    "        return defense_options\n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"One step in the simulation\"\"\"\n",
    "        done=False\n",
    "        reward=[0,0,0]\n",
    "        action_x = action_dict['agent_x']\n",
    "        action_o = action_dict['agent_o']\n",
    "        \n",
    "        if self.turn==self.PLAYER_X:\n",
    "            player_id,enemy_id = (self.PLAYER_X,self.PLAYER_O)\n",
    "            action_player,action_enemy = (action_x,action_o)\n",
    "        else:\n",
    "            player_id,enemy_id = (self.PLAYER_O,self.PLAYER_X)        \n",
    "            action_player,action_enemy = (action_o,action_x)\n",
    "        \n",
    "        # check movement current player        \n",
    "        if action_player == self.WAIT or self.board[action_player//3, action_player%3] != 0:\n",
    "            # invalid movement\n",
    "            reward[player_id] = -7\n",
    "        else:\n",
    "            # next time the enemy will play\n",
    "            self.turn = enemy_id\n",
    "            # valid movement\n",
    "            row,col=action_player//3,action_player%3\n",
    "            self.board[row, col] = player_id\n",
    "            # is it a critical situation that requires defense?\n",
    "            defense_options = self.__defense_detection__(self.board, enemy_id)            \n",
    "            if len(defense_options) > 0:\n",
    "                reward[player_id] = -10 # probably will lose if this is not a defense, lets see\n",
    "                for i in defense_options:\n",
    "                    if i[0]==row and i[1]==col: ## woohoo! defended\n",
    "                        reward[player_id] = 8\n",
    "                        break\n",
    "            else:\n",
    "                reward[player_id] = 1\n",
    "        \n",
    "        # Enemy should be waiting\n",
    "        if action_enemy != self.WAIT: reward[enemy_id] = -7\n",
    "        \n",
    "        tests = [np.diagonal(self.board), np.fliplr(self.board).diagonal()]\n",
    "        for i in range(3): tests += [self.board[i],self.board[:,i]]\n",
    "        # check board status\n",
    "        if   (np.array(tests)==player_id).all(axis=1).any(): done,reward[player_id] = True,15 # win\n",
    "        elif (np.array(tests)==enemy_id).all(axis=1).any(): done,reward[player_id] = True,-15 # defeat\n",
    "        elif not (np.array(tests)==0).any(): done,reward[player_id],reward[enemy_id] = True,2,2 # draw\n",
    "        elif self.trials < 1: done,reward[player_id],reward[enemy_id] = True,-5,-5\n",
    "        \n",
    "        self.trials -= 1        \n",
    "        \n",
    "        if done: self.render()\n",
    "\n",
    "        obs = {\n",
    "            'agent_x': np.concatenate([self.board.flatten(),[self.turn]], axis=0),\n",
    "            'agent_o': np.concatenate([self.board.flatten(),[self.turn]], axis=0)\n",
    "        }\n",
    "        reward = {'agent_x': reward[1], 'agent_o': reward[2]}\n",
    "        done = {'agent_x': done, 'agent_o': done, '__all__': done}\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def reset(self):        \n",
    "        self.trials = 20\n",
    "        self.turn = np.random.randint(1,3)\n",
    "        self.board = np.zeros((3,3), dtype=np.uint8)\n",
    "        obs = {\n",
    "            'agent_x': np.concatenate([self.board.flatten(),[self.turn]], axis=0),\n",
    "            'agent_o': np.concatenate([self.board.flatten(),[self.turn]], axis=0)\n",
    "        }\n",
    "        return obs\n",
    " \n",
    "    def render(self, mode='none', close=False):\n",
    "        if mode=='none': return\n",
    "        for i in range(9):\n",
    "            print(self.marker[self.board[i//3,i%3]], end='\\n' if i % 3 == 2 else ' ')\n",
    "        print()\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        print(f\"TicTacToeEnv - Seeding {seed}\")\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93ef3b",
   "metadata": {},
   "source": [
    "### 2/4) Then you need to create the heuristics/policy\n",
    "\n",
    "This policy will be used as an rival player to train the agent. It has a set of rules that are adjusted stochastically over time to be a harder or an easier player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tictactoe/heuristics.py\n",
    "from ray.rllib.policy.policy import Policy\n",
    "import random\n",
    "import numpy as np\n",
    "from tictactoe import TicTacToeEnv\n",
    "\n",
    "class SemiSmartTicTacToeHeuristicsPolicy(Policy):\n",
    "    \"\"\"Starts with random movements but tries to avoid defeat\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.exploration = self._create_exploration()\n",
    "        seed = args[2]['seed']\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def __attack_or_defend__(self, board, agent_id):\n",
    "        board_play=board==agent_id # agent id\n",
    "        board_mask=board==0 # empty cells\n",
    "        h = np.sum(board_play, axis=1)# horizontal\n",
    "        v = np.sum(board_play, axis=0) # vertical\n",
    "        # diagonals\n",
    "        diagA,diagAMask = np.diagonal(board_play),np.diagonal(board_mask)     \n",
    "        diagB,diagBMask = np.fliplr(board_play).diagonal(),np.fliplr(board_mask).diagonal()    \n",
    "        for idx,row in enumerate(h): # scan rows\n",
    "            if row==2 and np.sum(board_mask[idx])>0:             \n",
    "                return (idx,np.argmax(board_mask[idx]))\n",
    "        for idx,col in enumerate(v): # scan cols\n",
    "            if col==2 and np.sum(board_mask[:,idx])>0: \n",
    "                return (np.argmax(board_mask[:,idx]),idx)\n",
    "        if np.sum(diagA)==2 and np.sum(diagAMask)>0: # scan diagonal A\n",
    "            idx = np.argmax(diagAMask); return (idx,idx)\n",
    "        if np.sum(diagB)==2 and np.sum(diagBMask)>0: # scan diagonal B\n",
    "            idx = np.argmax(diagBMask); return (idx,2-idx)\n",
    "        return None, None\n",
    "        \n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        \n",
    "        def determine_action(obs):\n",
    "            # Wait if it's not player's turn.\n",
    "            if obs[9] == TicTacToeEnv.PLAYER_X: return 9\n",
    "            \n",
    "            board = obs[:-1].reshape((3,3))\n",
    "            if random.randint(0,3) == 0: # 33% hard - 66% potentially dumb\n",
    "                row,col = self.__attack_or_defend__(board, TicTacToeEnv.PLAYER_O) # attack\n",
    "                if row is not None: return (row*3)+col\n",
    "                row,col = self.__attack_or_defend__(board, TicTacToeEnv.PLAYER_X) # defend\n",
    "                if row is not None: return (row*3)+col\n",
    "                    \n",
    "            # Make a move on the first empty field heuristic can find.\n",
    "            empty_cells = []\n",
    "            for i, symbol in enumerate(obs):\n",
    "                if symbol == 0: empty_cells.append(i)\n",
    "            if len(empty_cells) > 0: return empty_cells[random.randint(0,len(empty_cells)-1)]\n",
    "            raise Exception('Heuristic did not find empty.')\n",
    "\n",
    "        return [determine_action(obs) for obs in obs_batch], [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ed8f4",
   "metadata": {},
   "source": [
    "### 3/4) Training the model\n",
    "SageMaker expects that you share a python script with the estimator to execute the training. The following script defines the whole training process using Ray+RLLib + Tensorflow 2 + OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tictactoe/train.py\n",
    "import sys\n",
    "import subprocess\n",
    "# we need a special package for cleaning our data, lets pip install it first\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"sagemaker-training==3.9.2\", \"ray[rllib]==1.2.0\"])\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import traceback\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.rllib as rllib\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register\n",
    "\n",
    "from sagemaker_training import environment, intermediate_output, logging_config, params, files\n",
    "\n",
    "from heuristics import SemiSmartTicTacToeHeuristicsPolicy\n",
    "\n",
    "def start_file_sync(env):\n",
    "    global logger, intermediate_sync\n",
    "    ## this service will copy all the files, stored in the intermediate dir, to S3\n",
    "    region = os.environ.get(\"AWS_REGION\", os.environ.get(params.REGION_NAME_ENV))\n",
    "    s3_endpoint_url = os.environ.get(params.S3_ENDPOINT_URL, None)\n",
    "\n",
    "    logger.info(\"Starting intermediate sync. %s: %s - %s\" % (region, env.sagemaker_s3_output(), s3_endpoint_url))\n",
    "    intermediate_sync = intermediate_output.start_sync(\n",
    "        env.sagemaker_s3_output(), region, endpoint_url=s3_endpoint_url\n",
    "    )\n",
    "    \n",
    "def get_latest_checkpoint(env, algo):\n",
    "    global logger\n",
    "    logger.info(\"Latest checkpoint\")\n",
    "    # get the latest experiment\n",
    "    experiments = glob.glob(os.path.join(env.output_intermediate_dir,'training', f'{algo}*'))\n",
    "    experiments.sort(key=lambda x: [int(c) if c.isdigit() else c for c in ''.join(x.replace('-','').split('_')[-2:])])\n",
    "\n",
    "    if len(experiments) > 0:\n",
    "        exp_name = experiments[-1]\n",
    "\n",
    "        chkpts = [c for c in glob.glob(f'{exp_name}/checkpoint*')]\n",
    "        chkpts.sort(key=lambda x: [int(c) if c.isdigit() else c for c in re.split('(\\d+)', x)])\n",
    "\n",
    "        if len(chkpts) == 0: raise Exception(\"No checkpoint found!\")\n",
    "        ckpt_path=chkpts[-1]\n",
    "        ckpt_meta_filename=ckpt_path.split('/')[-1].split('_')\n",
    "        ckpt_meta_filename=f'{ckpt_meta_filename[0]}-{int(ckpt_meta_filename[1])}'\n",
    "        logger.info(f'{ckpt_path}/{ckpt_meta_filename}')\n",
    "        return ckpt_path, ckpt_meta_filename\n",
    "\n",
    "def save_model(env_vars, experiment_params):\n",
    "    global logger\n",
    "    config = copy.deepcopy(experiment_params)['training']['config']\n",
    "\n",
    "    config[\"monitor\"] = False\n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"num_gpus\"] = 0\n",
    "    logger.info(experiment_params)\n",
    "    algo = experiment_params['training']['run']\n",
    "    env_name = experiment_params['training']['env']\n",
    "    logger.info(f'{algo} - {env_name}')\n",
    "    cls = get_agent_class(algo)        \n",
    "    agent = cls(env=env_name, config=config)\n",
    "    \n",
    "    ckpt_path, ckpt_meta_filename = get_latest_checkpoint(env_vars, algo)\n",
    "    \n",
    "    logger.info('Restoring agent...')\n",
    "    agent.restore(os.path.join(ckpt_path, ckpt_meta_filename))\n",
    "    logger.info(f'Exporting model to {env_vars.model_dir}...')\n",
    "    agent.export_policy_model(os.path.join(env_vars.model_dir, \"1\"), 'agent_x')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env_vars = environment.Environment()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    logging_config.configure_logger(env_vars.log_level)\n",
    "    \n",
    "    parser.add_argument(\"--log-level\", type=int, default=0)\n",
    "    parser.add_argument(\"--record-videos\", type=bool, default=False)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=max(env_vars.num_cpus-1, 3))\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=env_vars.num_gpus)\n",
    "    parser.add_argument(\"--batch-mode\", type=str, default=\"complete_episodes\")\n",
    "    parser.add_argument(\"--episode-reward-mean\", type=float, default=3.5)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--init-seed\", type=int, default=-1)\n",
    "    parser.add_argument(\"--refining-iter\", type=int, default=4)\n",
    "    args,unknown = parser.parse_known_args()\n",
    "\n",
    "    seed=args.init_seed if args.init_seed != -1 else None\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    logger = logging_config.get_logger()\n",
    "    intermediate_sync = None\n",
    "\n",
    "    env_name='TicTacToeEnv-v0'\n",
    "    register(\n",
    "        id=env_name,\n",
    "        entry_point='tictactoe:TicTacToeEnv'\n",
    "    )\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    experiment_params = {\n",
    "        \"training\": {\n",
    "            \"env\": env_name,\n",
    "            \"run\": \"A3C\",\n",
    "            \"stop\": {\n",
    "                \"episode_reward_mean\": args.episode_reward_mean,\n",
    "            },\n",
    "            \"local_dir\": env_vars.output_intermediate_dir,\n",
    "            \"checkpoint_at_end\": True,\n",
    "            \"checkpoint_freq\": 60,\n",
    "            #\"export_formats\": [\"h5\"],\n",
    "            \"config\": {            \n",
    "                \"log_level\": args.log_level,\n",
    "                \"monitor\": args.record_videos,\n",
    "                #\"framework\": \"tfe\",\n",
    "                \"lr\": args.learning_rate,\n",
    "                \"model\": {\n",
    "                    # https://docs.ray.io/en/master/rllib-models.html#default-model-config-settings\n",
    "                },\n",
    "                \"multiagent\": {\n",
    "                    \"policies\": {\n",
    "                        \"agent_x\": (None, env.observation_space, env.action_space, {}),\n",
    "                        \"agent_o\": (SemiSmartTicTacToeHeuristicsPolicy, env.observation_space, env.action_space, {})\n",
    "                    },\n",
    "                    \"policy_mapping_fn\": lambda x: x,\n",
    "                    \"policies_to_train\": [\"agent_x\"],                \n",
    "                },            \n",
    "                \"num_workers\": args.num_workers,\n",
    "                \"num_gpus\": args.num_gpus,\n",
    "                \"batch_mode\": args.batch_mode,\n",
    "                \"seed\": seed\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        start_file_sync(env_vars)\n",
    "        # main program\n",
    "        ray.init()\n",
    "        ray.tune.register_env(env_name, lambda x: env)\n",
    "        ray.tune.run_experiments(copy.deepcopy(experiment_params))\n",
    "        for i in range(args.refining_iter):\n",
    "            seed = int(time.time())\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)    \n",
    "            env.seed(seed)\n",
    "            algo = experiment_params['training']['run']\n",
    "            ckpt_path, ckpt_meta_filename = get_latest_checkpoint(env_vars, algo)\n",
    "            experiment_params['training']['config']['seed'] = seed\n",
    "            experiment_params['training']['restore'] = os.path.join(ckpt_path, ckpt_meta_filename)\n",
    "            ray.tune.run_experiments(copy.deepcopy(experiment_params))\n",
    "        save_model(env_vars, experiment_params)\n",
    "        ray.shutdown()\n",
    "        \n",
    "        files.write_success_file()\n",
    "        logger.info(\"Reporting training SUCCESS\")\n",
    "    except Exception as e:\n",
    "        failure_msg = \"framework error: \\n%s\\n%s\" % (traceback.format_exc(), str(e))\n",
    "        logger.error(\"Reporting training FAILURE: %s\" % failure_msg)\n",
    "        files.write_failure_file(failure_msg)\n",
    "    finally:\n",
    "        if intermediate_sync:\n",
    "            intermediate_sync.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66d906",
   "metadata": {},
   "source": [
    "#### Training using SageMaker RL container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "# S3 bucket\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "\n",
    "# create a descriptive job name \n",
    "aws_region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "import time\n",
    "\n",
    "image_name=f\"462105765813.dkr.ecr.{aws_region}.amazonaws.com/sagemaker-rl-ray-container:ray-1.1.0-tf-gpu-py36\"\n",
    "estimator = RLEstimator(\n",
    "    image_uri=image_name,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir='tictactoe',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    #instance_type='local_gpu',\n",
    "    max_run=60*(60 * 2),\n",
    "    instance_count=1,\n",
    "    output_path=s3_output_path,\n",
    "    metric_definitions=RLEstimator.default_metric_definitions(RLToolkit.RAY),\n",
    "    hyperparameters={\n",
    "        \"log-level\": 20,\n",
    "        \"record-videos\": False,\n",
    "        \"batch-mode\": \"complete_episodes\",\n",
    "        \"episode-reward-mean\": 5.0,\n",
    "        \"learning-rate\": 0.0001,\n",
    "        \"init-seed\": 1, # seed == 1 makes the agent learn faster but it gets biased\n",
    "        \"refining-iter\": 4 # refining iterations are to make the agent generalize to random matches\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323eaba9",
   "metadata": {},
   "source": [
    "#### Kick-off the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce49583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf /tmp/tmp*\n",
    "estimator.fit(wait=True)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff037f4",
   "metadata": {},
   "source": [
    "### 4/4) Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri=f'{estimator.output_path}{estimator.latest_training_job.name}/output/model.tar.gz'\n",
    "print(s3_uri)\n",
    "!aws s3 cp $s3_uri /tmp/\n",
    "!mkdir -p model\n",
    "!tar -xzvf /tmp/model.tar.gz -C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2205cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# loading\n",
    "model_version = '1'\n",
    "model_dir = 'model'\n",
    "export_dir = os.path.join(model_dir, model_version)\n",
    "with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "    tf.compat.v1.saved_model.loader.load(sess, [tag_constants.SERVING], export_dir)\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    \n",
    "    #for o in graph.get_operations(): print(o.name)    \n",
    "    x = graph.get_tensor_by_name('agent_x/observations:0')    \n",
    "    y = graph.get_tensor_by_name('agent_x/fc_out/BiasAdd:0')\n",
    "    #obs = [2,2,1,0,1,0,0,0,0,1]\n",
    "    #obs = [0,0,0,0,0,0,0,0,0,1]\n",
    "    obs = [2,0,1,0,2,0,0,0,0,1]\n",
    "    payload = np.array([obs], dtype=np.float32)\n",
    "    preds = sess.run(y, feed_dict={x: payload})\n",
    "    \n",
    "    print(payload[0][:-1].reshape((3,3)))\n",
    "    print(np.argmax(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f67a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
