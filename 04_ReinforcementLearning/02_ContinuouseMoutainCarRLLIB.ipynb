{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mountain && mkdir -p mountain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mountain/train.py\n",
    "import sys\n",
    "import subprocess\n",
    "# we need a special package for cleaning our data, lets pip install it first\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker-training==3.9.2\", \"pyglet==1.5.16\"])\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "import argparse\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import gym\n",
    "import ray\n",
    "import ray.tune\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from shutil import copyfile\n",
    "from sagemaker_training import environment, intermediate_output, logging_config, params, files\n",
    "\n",
    "def start_file_sync(env):\n",
    "    global logger, intermediate_sync\n",
    "    ## this service will copy all the files, stored in the intermediate dir, to S3\n",
    "    region = os.environ.get(\"AWS_REGION\", os.environ.get(params.REGION_NAME_ENV))\n",
    "    s3_endpoint_url = os.environ.get(params.S3_ENDPOINT_URL, None)\n",
    "\n",
    "    logger.info(\"Starting intermediate sync. %s: %s - %s\" % (region, env.sagemaker_s3_output(), s3_endpoint_url))\n",
    "    intermediate_sync = intermediate_output.start_sync(\n",
    "        env.sagemaker_s3_output(), region, endpoint_url=s3_endpoint_url\n",
    "    )\n",
    "    \n",
    "def get_latest_checkpoint(env, algo):\n",
    "    global logger\n",
    "    logger.info(\"Latest checkpoint\")\n",
    "    # get the latest experiment\n",
    "    experiments = glob.glob(os.path.join(env.output_intermediate_dir,'training', f'{algo}*'))\n",
    "    experiments.sort(key=lambda x: [int(c) if c.isdigit() else c for c in ''.join(x.replace('-','').split('_')[-2:])])\n",
    "\n",
    "    if len(experiments) > 0:\n",
    "        exp_name = experiments[-1]\n",
    "\n",
    "        chkpts = [c for c in glob.glob(f'{exp_name}/checkpoint*')]\n",
    "        chkpts.sort(key=lambda x: [int(c) if c.isdigit() else c for c in re.split('(\\d+)', x)])\n",
    "\n",
    "        if len(chkpts) == 0: raise Exception(\"No checkpoint found!\")\n",
    "        ckpt_path=chkpts[-1]\n",
    "        ckpt_meta_filename=ckpt_path.split('/')[-1].split('_')\n",
    "        ckpt_meta_filename=f'{ckpt_meta_filename[0]}-{int(ckpt_meta_filename[1])}'\n",
    "        logger.info(f'{ckpt_path}/{ckpt_meta_filename}')\n",
    "        return ckpt_path, ckpt_meta_filename\n",
    "\n",
    "def save_model(env_vars, experiment_params):\n",
    "    global logger\n",
    "    config = copy.deepcopy(experiment_params)['training']['config']\n",
    "\n",
    "    config[\"monitor\"] = False\n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"num_gpus\"] = 0\n",
    "    logger.info(experiment_params)\n",
    "    algo = experiment_params['training']['run']\n",
    "    env_name = experiment_params['training']['env']\n",
    "    logger.info(f'{algo} - {env_name}')\n",
    "    cls = get_agent_class(algo)        \n",
    "    agent = cls(env=env_name, config=config)\n",
    "    \n",
    "    ckpt_path, ckpt_meta_filename = get_latest_checkpoint(env_vars, algo)\n",
    "    \n",
    "    logger.info('Restoring agent...')\n",
    "    agent.restore(os.path.join(ckpt_path, ckpt_meta_filename))\n",
    "    logger.info('Exporting model...')\n",
    "    agent.export_policy_model(os.path.join(env_vars.model_dir, \"1\"))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    env_vars = environment.Environment()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    logging_config.configure_logger(env_vars.log_level)\n",
    "    \n",
    "    parser.add_argument(\"--eager\", type=bool, default=False)\n",
    "    parser.add_argument(\"--log-level\", type=int, default=0)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.995)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--kl-coeff\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--num-sgd-iter\", type=int, default=20)\n",
    "    parser.add_argument(\"--sgd-minibatch-size\", type=int, default=1000)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=25000)\n",
    "    parser.add_argument(\"--record-videos\", type=bool, default=False)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=max(env_vars.num_cpus-1, 3))\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=env_vars.num_gpus)\n",
    "    parser.add_argument(\"--batch-mode\", type=str, default=\"complete_episodes\")\n",
    "    parser.add_argument(\"--episode-reward-mean\", type=int, default=18)\n",
    "    args,unknown = parser.parse_known_args()\n",
    "    \n",
    "    logger = logging_config.get_logger()\n",
    "    intermediate_sync = None\n",
    "\n",
    "    env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "    experiment_params = {\n",
    "        \"training\": {\n",
    "            \"env\": env_name,\n",
    "            \"run\": \"PPO\",\n",
    "            \"stop\": {\n",
    "                \"episode_reward_mean\": args.episode_reward_mean,\n",
    "            },\n",
    "            \"local_dir\": env_vars.output_intermediate_dir,\n",
    "            \"checkpoint_at_end\": True,\n",
    "            \"checkpoint_freq\": 10,\n",
    "            \"config\": {\n",
    "                \"log_level\": args.log_level,\n",
    "                \"gamma\": args.gamma,\n",
    "                \"kl_coeff\": args.kl_coeff,\n",
    "                \"num_sgd_iter\": args.num_sgd_iter,\n",
    "                \"lr\": args.learning_rate,\n",
    "                \"sgd_minibatch_size\": args.sgd_minibatch_size,\n",
    "                \"train_batch_size\": args.train_batch_size,\n",
    "                \"monitor\": args.record_videos,\n",
    "                \"model\": {\n",
    "                    # https://docs.ray.io/en/master/rllib-models.html#default-model-config-settings\n",
    "                    \"free_log_std\": True\n",
    "                },\n",
    "                \"num_workers\": args.num_workers,\n",
    "                \"num_gpus\": args.num_gpus,\n",
    "                \"batch_mode\": args.batch_mode\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_file_sync(env_vars)\n",
    "        # main program\n",
    "        ray.init()\n",
    "        ray.tune.register_env(env_name, lambda x: gym.make(env_name))\n",
    "        ray.tune.run_experiments(copy.deepcopy(experiment_params))\n",
    "        save_model(env_vars, experiment_params)\n",
    "        ray.shutdown()\n",
    "        \n",
    "        files.write_success_file()\n",
    "        logger.info(\"Reporting training SUCCESS\")\n",
    "    except Exception as e:\n",
    "        failure_msg = \"framework error: \\n%s\\n%s\" % (traceback.format_exc(), str(e))\n",
    "        logger.error(\"Reporting training FAILURE: %s\" % failure_msg)\n",
    "\n",
    "        files.write_failure_file(failure_msg)\n",
    "    finally:\n",
    "        if intermediate_sync:\n",
    "            intermediate_sync.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "# S3 bucket\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "\n",
    "# create a descriptive job name \n",
    "aws_region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "\n",
    "image_name=\"462105765813.dkr.ecr.us-east-1.amazonaws.com/sagemaker-rl-ray-container:ray-1.1.0-tf-gpu-py36\"\n",
    "estimator = RLEstimator(\n",
    "    image_uri=image_name,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir='mountain',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    #instance_type='local_gpu',\n",
    "    instance_count=1,\n",
    "    output_path=s3_output_path,\n",
    "    metric_definitions=RLEstimator.default_metric_definitions(RLToolkit.RAY),\n",
    "    hyperparameters={\n",
    "        \"log-level\": 20,\n",
    "        \"gamma\": 0.995,\n",
    "        \"learning-rate\": 0.0001,\n",
    "        \"kl-coeff\": 1.0,\n",
    "        \"num-sgd-iter\": 20,\n",
    "        \"sgd-minibatch-size\": 1000,\n",
    "        \"rain-batch-size\": 25000,\n",
    "        \"record-videos\": True,\n",
    "        \"batch-mode\": \"complete_episodes\",\n",
    "        \"episode-reward-mean\": 80\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf /tmp/tmp*\n",
    "estimator.fit(wait=True)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Output.tar.gz location: {}\".format(output_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from IPython.display import Video, display\n",
    "\n",
    "path = \"%s%s\" % (intermediate_url, \"training/\")\n",
    "!rm -rf $tmp_dir\n",
    "!aws s3 sync --quiet $path $tmp_dir\n",
    "\n",
    "sub_dir = []\n",
    "for i in os.listdir(tmp_dir): \n",
    "    full_path=os.path.join(tmp_dir, i)\n",
    "    if os.path.isdir(full_path): sub_dir.append(full_path)\n",
    "if len(sub_dir) > 0:\n",
    "    video_path = sorted(glob.glob(sub_dir[0] + '/*.mp4'))[-1]\n",
    "    print(video_path)\n",
    "    !cp $video_path video.mp4\n",
    "    display(Video(\"video.mp4\"))\n",
    "else:\n",
    "    print('There is no available video yet...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
