{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logo detection using YoloV2 and Darknet on Amazon SageMaker\n",
    "\n",
    "This example shows how you create a Darknet image https://pjreddie.com/darknet/ for Amazon SageMaker. With this image you can **train** and **deploy** ML models.\n",
    "\n",
    "There are 3 exercises in total:\n",
    "  - This is the first one, where you'll create a docker image to work with Darknet on SageMaker\n",
    "  - In the second exercise you'll prepare the dataset [Openlogo](https://qmul-openlogo.github.io/index.html)\n",
    "  - Finally you'll train/deploy and test the logo detector\n",
    "\n",
    "SageMaker provides libraries that we can use to help us to create the Docker image:\n",
    "  - https://github.com/aws/sagemaker-inference-toolkit\n",
    "  - https://github.com/aws/sagemaker-training-toolkit\\\n",
    " \n",
    "So, let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf container && mkdir -p container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0) First, let's use SageMaker Inference toolkit and create a handler for the predictions\n",
    "This class will be used by SageMaker when an it is time to run the model(prediction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/handler.py\n",
    "import os\n",
    "import sys\n",
    "import darknet as dn\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "from ctypes import pointer,c_int\n",
    "from sagemaker_inference.default_inference_handler import DefaultInferenceHandler\n",
    "from sagemaker_inference.default_handler_service import DefaultHandlerService\n",
    "from sagemaker_inference import content_types, errors, transformer, encoder, decoder\n",
    "\n",
    "class HandlerService(DefaultHandlerService, DefaultInferenceHandler):\n",
    "    def __init__(self):\n",
    "        op = transformer.Transformer(default_inference_handler=self)\n",
    "        super(HandlerService, self).__init__(transformer=op)\n",
    "        self.thresh=.5\n",
    "        self.hier_thresh=.5\n",
    "        self.nms=.45\n",
    "        self.num_classes = 335\n",
    "\n",
    "    ## Loads the model from the disk\n",
    "    def default_model_fn(self, model_dir):\n",
    "        cfg_path = os.path.join(model_dir, \"model.cfg\").encode('utf-8')\n",
    "        model_path = os.path.join(model_dir, \"model.weights\").encode('utf-8')\n",
    "\n",
    "        return dn.load_net(cfg_path, model_path, 0)\n",
    "\n",
    "    ## Parse and check the format of the input data\n",
    "    def default_input_fn(self, input_data, content_type):\n",
    "        if not content_type in [\"image/jpeg\", \"image/png\" ]:\n",
    "            raise Exception(\"Invalid content-type: %s\" % content_type)\n",
    "        img = np.array(Image.open(io.BytesIO(input_data)))\n",
    "        # now lets create a cbinding image\n",
    "        h, w, c = img.shape\n",
    "        im = dn.make_image(w, h, c)\n",
    "\n",
    "        img = np.divide(np.rollaxis(img, axis=2, start=0).flatten(), 255.)\n",
    "        for i in range(h*w*c):\n",
    "            im.data[i] = img[i]\n",
    "\n",
    "        return im\n",
    "\n",
    "    ## Run our model and do the prediction\n",
    "    def default_predict_fn(self, payload, model):\n",
    "        num = c_int(0)\n",
    "        pnum = pointer(num)\n",
    "        a = dn.predict_image(model, payload)\n",
    "        print(a[0])\n",
    "        dets = dn.get_network_boxes(model, payload.w, payload.h, self.thresh, self.hier_thresh, None, 0, pnum)\n",
    "        num = pnum[0]\n",
    "        if (self.nms): dn.do_nms_obj(dets, num, self.num_classes, self.nms)\n",
    "        res = []\n",
    "        for j in range(num):\n",
    "            for i in range(self.num_classes):\n",
    "                if dets[j].prob[i] > 0:\n",
    "                    b = dets[j].bbox\n",
    "                    res.append((i, dets[j].prob[i], (b.x, b.y, b.w, b.h)))\n",
    "        res = sorted(res, key=lambda x: -x[1])\n",
    "        dn.free_image(payload)\n",
    "        dn.free_detections(dets, num)\n",
    "        return res\n",
    "\n",
    "    ## Gets the prediction output and format it to be returned to the user\n",
    "    def default_output_fn(self, prediction, accept):\n",
    "        if accept != \"application/json\":\n",
    "            raise Exception(\"Invalid accept: %s\" % accept)\n",
    "        return encoder.encode(prediction, accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Now we need to create the container entrypoint\n",
    "\n",
    "This script will **handle** both training and predictions. So, we need to check the command (train or serve) and execute the appropriate code for each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/main.py\n",
    "import argparse\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sagemaker_inference import model_server\n",
    "from sagemaker_training import environment, intermediate_output, params, logging_config, files\n",
    "\n",
    "logger = logging_config.get_logger()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\" ] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "        \n",
    "    if sys.argv[1] == \"train\":\n",
    "        \n",
    "        env = environment.Environment()\n",
    "        parser = argparse.ArgumentParser()\n",
    "        logging_config.configure_logger(env.log_level)\n",
    "        logger.info( \"Starting a new training! %s\" % env.log_level)\n",
    "        # https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md\n",
    "\n",
    "        # reads input channels training and testing from the environment variables\n",
    "        parser.add_argument(\"--training\", type=str, default=env.channel_input_dirs[\"training\"])\n",
    "        parser.add_argument(\"--testing\", type=str, default=env.channel_input_dirs[\"testing\"])\n",
    "        parser.add_argument(\"--assets\", type=str, default=env.channel_input_dirs[\"assets\"])\n",
    "\n",
    "        parser.add_argument(\"--model-dir\", type=str, default=env.model_dir)\n",
    "        parser.add_argument(\"--checkpoints-dir\", type=str, default=env.output_intermediate_dir)\n",
    "\n",
    "        parser.add_argument(\"--num-classes\", type=int, default=env.hyperparameters.get(\"num_classes\"))\n",
    "        parser.add_argument(\"--cfg\", type=str, default=env.hyperparameters.get(\"cfg\"))\n",
    "        parser.add_argument(\"--weights\", type=str, default=env.hyperparameters.get(\"weights\"))\n",
    "\n",
    "        parser.add_argument(\"--train-file\", type=str, default=env.hyperparameters.get(\"train_file\"))\n",
    "        parser.add_argument(\"--test-file\", type=str, default=env.hyperparameters.get(\"test_file\"))\n",
    "        parser.add_argument(\"--names-file\", type=str, default=env.hyperparameters.get(\"names_file\"))\n",
    "\n",
    "        args,unknown = parser.parse_known_args()\n",
    "\n",
    "        logger.info(\"ENV: %s\" % (env) )\n",
    "        logger.info(\"ARGS: %s\" % (args) )\n",
    "        \n",
    "        command = [\"darknet\", \"detector\", \"train\", \"/tmp/temp.data\"]\n",
    "\n",
    "        if args.cfg is None or not os.path.isfile(os.path.join(args.assets, args.cfg)):\n",
    "            raise Exception(\"You need to inform a valid .cfg file: %s\" % args.cfg)\n",
    "        command.append( os.path.join(args.assets, args.cfg) )\n",
    "\n",
    "        if args.weights is not None:\n",
    "            weights_file = os.path.join(args.assets,  args.weights )\n",
    "            if not os.path.isfile(weights_file):\n",
    "                raise Exception('You defined an invalid weights file')\n",
    "            command.append(weights_file)\n",
    "\n",
    "        train_file = os.path.join(args.training, args.train_file)\n",
    "        test_file = os.path.join(args.testing, args.test_file)\n",
    "        names_file = os.path.join(args.assets,  args.names_file )\n",
    "        model_prefix = os.path.join(args.checkpoints_dir, args.cfg.split('.')[0])\n",
    "        model_cfg_filename = os.path.join(args.assets, args.cfg)\n",
    "\n",
    "        subprocess.call(['sed', '-i', '-e', 's#^#%s/#' % args.training, train_file])\n",
    "        subprocess.call(['sed', '-i', '-e', 's#^#%s/#' % args.testing, test_file])\n",
    "\n",
    "        with open('/tmp/temp.data', 'w') as f:\n",
    "            f.write(\"classes=%d\\n\" % args.num_classes)\n",
    "            f.write(\"train=%s\\n\" % train_file)\n",
    "            f.write(\"valid=%s\\n\" % test_file)\n",
    "            f.write(\"names=%s\\n\" % names_file )\n",
    "            f.write(\"backup=%s\\n\" % args.checkpoints_dir)\n",
    "\n",
    "        gpus = ','.join([str(i) for i in range(env.num_gpus)])\n",
    "        if gpus != '': \n",
    "            command += [\"-gpus\", gpus]\n",
    "        logger.info(command)\n",
    "        \n",
    "        intermediate_sync = None\n",
    "        try:\n",
    "            region = os.environ.get(\"AWS_REGION\", os.environ.get(params.REGION_NAME_ENV))\n",
    "            s3_endpoint_url = os.environ.get(params.S3_ENDPOINT_URL, None)\n",
    "            logger.info(\"Starting intermediate sync. %s: %s - %s\" % (region, env.sagemaker_s3_output(), s3_endpoint_url))\n",
    "            intermediate_sync = intermediate_output.start_sync(\n",
    "                env.sagemaker_s3_output(), region, endpoint_url=s3_endpoint_url\n",
    "            )\n",
    "            logger.info(intermediate_sync)\n",
    "            subprocess.call(command)\n",
    "            new_model_cfg = os.path.join(args.model_dir, \"model.cfg\")\n",
    "            subprocess.call([\"cp\", model_cfg_filename, new_model_cfg])\n",
    "            subprocess.call([\"mv\", \"%s_final.weights\" % model_prefix, os.path.join(args.model_dir, \"model.weights\")])\n",
    "\n",
    "            # we need to set batch and subdivisions to 1 to accept 1 image per prediction\n",
    "            subprocess.call(['sed', '-i', '-e', 's#^batch\\s*=\\s*[0-9]\\+#batch=1#', new_model_cfg])\n",
    "            subprocess.call(['sed', '-i', '-e', 's#^subdivisions\\s*=\\s*[0-9]\\+#subdivisions=1#', new_model_cfg])\n",
    "            \n",
    "            logger.info(\"Reporting training SUCCESS\")\n",
    "            files.write_success_file()\n",
    "        except Exception as e:\n",
    "            failure_msg = \"framework error: \\n%s\\n%s\" % (traceback.format_exc(), str(e))\n",
    "            logger.error(\"Reporting training FAILURE\")\n",
    "            logger.error(failure_msg)\n",
    "            files.write_failure_file(failure_msg)\n",
    "        finally:\n",
    "            if intermediate_sync:\n",
    "                intermediate_sync.join()\n",
    "    else:\n",
    "        model_server.start_model_server(handler_service=\"serving.handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Finally we need to create the Dockerfile to prepare our container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/Dockerfile\n",
    "FROM nvidia/cuda:10.1-cudnn7-devel\n",
    "\n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "ENV TZ=Europe/London\n",
    "\n",
    "RUN apt-get update -y && apt-get -y install \\\n",
    "    --no-install-recommends default-jdk build-essential git python3.6 python3.6-dev python3-pip\n",
    "\n",
    "RUN apt-get clean && rm -rf /var/cache/apt && \\\n",
    "    apt-get -y autoremove && apt-get -y autoclean && \\\n",
    "    rm -rf /var/cache/apt /var/lib/apt/lists/*\n",
    "\n",
    "RUN mkdir -p /opt/ml/code\n",
    "RUN git clone https://github.com/pjreddie/darknet.git && \\\n",
    "    sed -i 's#GPU=0#GPU=1#' darknet/Makefile && \\\n",
    "    sed -i 's#CUDNN=0#CUDNN=1#' darknet/Makefile && \\\n",
    "    cd darknet && make -j && \\\n",
    "    mv darknet /usr/bin && \\\n",
    "    mkdir -p /opt/ml/code && \\\n",
    "    mv python/darknet.py /opt/ml/code && \\\n",
    "    mv libdarknet.so /usr/lib && \\\n",
    "    ldconfig\n",
    "RUN rm -rf darknet\n",
    "\n",
    "RUN pip3 --no-cache-dir install -U setuptools \n",
    "RUN pip3 --no-cache-dir install -U multi-model-server sagemaker-inference sagemaker-training 2to3 wheel\n",
    "\n",
    "RUN 2to3 -w /opt/ml/code/darknet.py\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PYTHONPATH=\"/opt/ml/code:${PATH}\"\n",
    "\n",
    "COPY main.py /opt/ml/code/main.py\n",
    "COPY handler.py /opt/ml/code/serving/handler.py\n",
    "\n",
    "# Defines train.py as script entry point\n",
    "#ENV SAGEMAKER_PROGRAM main.py\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/opt/ml/code/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0) We can use the local Docker daemon to build our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t darknet:latest container/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0) Upload the image to ECR\n",
    "### Pushing the image to ECR\n",
    "Before executing the next cell, go to ECR and create a new repo, called **darknet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [715445047862.dkr.ecr.us-east-1.amazonaws.com/darknet]\n",
      "\n",
      "\u001b[1B93e10553: Preparing \n",
      "\u001b[1Baf79b19b: Preparing \n",
      "\u001b[1Bd3f5bb73: Preparing \n",
      "\u001b[1B13417d2a: Preparing \n",
      "\u001b[1Baaf5e3b8: Preparing \n",
      "\u001b[1B10cac599: Preparing \n",
      "\u001b[1Bfff24be3: Preparing \n",
      "\u001b[1B87a7da47: Preparing \n",
      "\u001b[1Ba6c7a448: Preparing \n",
      "\u001b[1B44daba9c: Preparing \n",
      "\u001b[1B9913a256: Preparing \n",
      "\u001b[1B2f599fd6: Preparing \n",
      "\u001b[1B74f76be4: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[11B0cac599: Waiting g \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[12Bff24be3: Waiting g \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[20B3e10553: Pushed lready exists 9kB6A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[19A\u001b[1K\u001b[K\u001b[20A\u001b[1K\u001b[K1.0: digest: sha256:272e229e2be9caf64f6d0a3a318d00c8f453b51ee5e900e268fede423eda988f size: 4513\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "!$(aws ecr get-login --no-include-email --region $region)\n",
    "!docker tag darknet:latest \"$account_id\".dkr.ecr.\"$region\".amazonaws.com/darknet:1.0\n",
    "!docker push \"$account_id\".dkr.ecr.\"$region\".amazonaws.com/darknet:1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
