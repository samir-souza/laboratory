{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620e83b1",
   "metadata": {},
   "source": [
    "# I/O Benchmark - SageMaker Processing Jobs with Pipe mode\n",
    "In this example, you'll verify how long does SageMaker take to transfer big files from S3 to the Instance Memory using Pipe Mode.\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\n",
    "\n",
    "In Pipe mode, SageMaker maps an S3 bucket with a Pipe inside the docker container created to run the job. That way, when the application reads the pipe, it streams the files from S3 using a very optimized mechanism. This is important specially if you're using really big files like in this example: 10x ~28GB .npy files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5030fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker default bucket: sagemaker-us-east-1-715445047862\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import os\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_s3_bucket = sagemaker_session.default_bucket()\n",
    "print(f'SageMaker default bucket: {default_s3_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7159e",
   "metadata": {},
   "source": [
    "### Generating big files\n",
    "### You can skip this if you already generated/uploaded the files\n",
    "#### You need to run this cell on an instance with more than 35GB of Ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's generate 1 bigfile ~29GB, upload it to S3 and replicate it\n",
    "if not os.path.isfile('big_file.npy'):\n",
    "    print('Generating a new big file...')\n",
    "    d = np.ones((10000, 3, 1280, 768), dtype=np.uint8)\n",
    "    np.save('big_file.npy', d)\n",
    "    print('Done!')\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Uploading file big_file_0{i}.npy to S3...')\n",
    "    s3_uri = f's3://{default_s3_bucket}/bigfiles/big_file_0{i}.npy'\n",
    "    subprocess.run(f\"aws s3 cp big_file.npy {s3_uri}\".split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0392da",
   "metadata": {},
   "source": [
    "### Processing script\n",
    "This script will be executed by SageMaker. It will read the Pipe until it gets all the available bytes. In Pipe mode, we don't know when one file ends and another starts. However, we know that the files are numpy .npy files. So, we can read the header and get the size of the body. Using this strategy, we'll get the list of files from the manifest (a virtual file created by SageMaker with the list of all files) and then iterate across all files, reading headers and then the payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a9bb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "input_dir = \"/opt/ml/processing/input\"\n",
    "output_dir = \"/opt/ml/processing/output\"\n",
    "failure_file = output_dir + \"/failure\"\n",
    "\n",
    "## helper function that gets the header metadata\n",
    "def read_array_header(fobj):\n",
    "    version = np.lib.format.read_magic(fobj)\n",
    "    func_name = 'read_array_header_' + '_'.join(str(v) for v in version)    \n",
    "    func = getattr(np.lib.format, func_name)\n",
    "    return func(fobj)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # parse the input parameters passed via SageMaker Python SDK\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--you-can-pass-parameters-like-this\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    try:\n",
    "        # list the pipes in the input dir\n",
    "        print(os.listdir(input_dir))\n",
    "        # parse the manifest file and extract all file names\n",
    "        file_names = [f.strip() for f in open(f'{input_dir}/input-1-manifest', 'r').readlines()[1:]] # skip first line\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        total_bytes = 0\n",
    "        # read the pipe\n",
    "        with open(f'{input_dir}/input-1_0', 'rb') as data:\n",
    "            # iterate over all file names and read only the bytes that belong to each file\n",
    "            for fname in file_names:\n",
    "                print(f'Reading header of {fname} - {time.time()}')\n",
    "                shape,_,dtype = read_array_header(data)\n",
    "                file_size = np.prod(shape) * dtype.itemsize\n",
    "                total_bytes += file_size\n",
    "                print(f'Reading body of {fname} - {time.time()}')\n",
    "                start_time = time.time()\n",
    "                payload = data.read(file_size)\n",
    "                ### --> Here you convert the payload into tensor, invoke the model and save \n",
    "                ### --> the predictions to output_dir                \n",
    "                print(f'Elapsed time: {time.time()-start_time}, Bytes: {file_size}')\n",
    "        print(f'Total bytes read: {total_bytes}. Total Elapsed time: {time.time()-total_start_time}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failed to train: %s\" % (sys.exc_info()[0]))\n",
    "        with open(failure_file, 'w') as f:\n",
    "            f.write(str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcb03c",
   "metadata": {},
   "source": [
    "### Now, let's run the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8fae840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# We'll use a pre-defined container with SKLearn to avoid having to build our own container\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type='ml.c5.9xlarge',\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1279d",
   "metadata": {},
   "source": [
    "### Execute the processin job\n",
    "Now that we have the processing script, invoke it to compute the I/O times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d29247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-07-14-14-44-36-548\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-715445047862/bigfiles', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'Pipe', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-715445047862/sagemaker-scikit-learn-2022-07-14-14-44-36-548/input/code/inference.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-715445047862/sagemaker-scikit-learn-2022-07-14-14-44-36-548/output/output-1', 'LocalPath': '/opt/ml/processing/output/predictions', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..............................\u001b[34mReceived arguments Namespace(you_can_pass_parameters_like_this=0.1)\u001b[0m\n",
      "\u001b[34m['input-1-manifest', 'input-1_1', 'code', 'input-1_0']\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_00.npy - 1657810160.9651217\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_00.npy - 1657810160.9681213\u001b[0m\n",
      "\u001b[34mElapsed time: 27.762173175811768, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_01.npy - 1657810188.7303476\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_01.npy - 1657810188.730845\u001b[0m\n",
      "\u001b[34mElapsed time: 31.342713832855225, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_02.npy - 1657810220.0736194\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_02.npy - 1657810220.0740042\u001b[0m\n",
      "\u001b[34mElapsed time: 28.18266749382019, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_03.npy - 1657810248.2567303\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_03.npy - 1657810248.2572792\u001b[0m\n",
      "\u001b[34mElapsed time: 28.09432029724121, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_04.npy - 1657810276.351673\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_04.npy - 1657810276.3520963\u001b[0m\n",
      "\u001b[34mElapsed time: 28.22616147994995, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_05.npy - 1657810304.5783153\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_05.npy - 1657810304.578718\u001b[0m\n",
      "\u001b[34mElapsed time: 28.75592851638794, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_06.npy - 1657810333.334709\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_06.npy - 1657810333.335093\u001b[0m\n",
      "\u001b[34mElapsed time: 27.61968994140625, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_07.npy - 1657810360.9548614\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_07.npy - 1657810360.955258\u001b[0m\n",
      "\u001b[34mElapsed time: 28.173229694366455, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_08.npy - 1657810389.128546\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_08.npy - 1657810389.1289384\u001b[0m\n",
      "\u001b[34mElapsed time: 28.58035111427307, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mReading header of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_09.npy - 1657810417.7093475\u001b[0m\n",
      "\u001b[34mReading body of s3://sagemaker-us-east-1-715445047862/bigfiles/big_file_09.npy - 1657810417.7097478\u001b[0m\n",
      "\u001b[34mElapsed time: 28.161729097366333, Bytes: 29491200000\u001b[0m\n",
      "\u001b[34mTotal bytes read: 294912000000. Total Elapsed time: 284.90645599365234\u001b[0m\n",
      "\n",
      "CPU times: user 994 ms, sys: 40 ms, total: 1.03 s\n",
      "Wall time: 10min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sklearn_processor.run(\n",
    "    code='inference.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        source=f's3://{default_s3_bucket}/bigfiles',\n",
    "        destination='/opt/ml/processing/input',\n",
    "        s3_input_mode='Pipe',\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )],\n",
    "    outputs=[ProcessingOutput(\n",
    "        source='/opt/ml/processing/output/predictions'),        \n",
    "    ],\n",
    "    arguments=[\"--you-can-pass-parameters-like-this\", \"0.1\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
