{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d416726",
   "metadata": {},
   "source": [
    "# MarianMT - Pytorch\n",
    "This notebook shows how to compile a pre-trainded MarianMT/Pytorch to AWS Inferentia (inf1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "**Reference:** https://huggingface.co/Helsinki-NLP/opus-mt-en-de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea195b",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pip repository  to point to the Neuron repository\n",
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc531ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron PyTorch\n",
    "%pip install -U torch-neuron==1.7.* \"protobuf<4\" \"transformers==4.0.1\" neuron-cc[tensorflow] sentencepiece\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e94754",
   "metadata": {},
   "source": [
    "## 2) Initialize libraries and prepare input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3483c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not \"..\" in sys.path: sys.path.append(\"..\")\n",
    "    \n",
    "from transformers import MarianMTModel, MarianTokenizer, MarianConfig\n",
    "\n",
    "model_name='Helsinki-NLP/opus-mt-en-de'   # English -> German model\n",
    "num_texts = 1                             # Number of input texts to decode\n",
    "num_beams = 4                             # Number of beams per input text\n",
    "max_encoder_length = 32                   # Maximum input token length\n",
    "max_decoder_length = 32                   # Maximum output token length\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text='I am a small frog'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c268ce",
   "metadata": {},
   "source": [
    "## 3) Load a pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d45837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "from common.wrapper import infer, NeuronGeneration\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "infer(model, tokenizer, text, num_beams, max_encoder_length, max_decoder_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fbb73",
   "metadata": {},
   "source": [
    "## 4) Compile the model for Inferentia with NeuronSDK\n",
    "\n",
    "This model is very complex, so we'll use a wrapper around the decoder and encoder sub-modules. This wrapper was  extracted [from this implementation](https://github.com/aws/aws-neuron-sdk/blob/master/src/examples/pytorch/transformers-marianmt.ipynb) to make the model traceable.\n",
    "\n",
    "For more details, please check the wrapper source code: [wrapper.py](../common/wrapper.py)\n",
    "\n",
    "The PyTorch-Neuron trace Python API provides a method to generate PyTorch models for execution on Inferentia, which can be serialized as TorchScript. It is analogous to torch.jit.trace() function in PyTorch.\n",
    "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-compilation-python-api.html?highlight=trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55867d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_neuron = NeuronGeneration(model.config)\n",
    "neuron_name='marianmt_en2gb'\n",
    "# 1. Compile the model\n",
    "# Note: This may take a couple of minutes since both the encoder/decoder will be compiled\n",
    "model_neuron.trace(\n",
    "    model=model,\n",
    "    num_texts=num_texts,\n",
    "    num_beams=num_beams,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_decoder_length=max_decoder_length,\n",
    ")\n",
    "\n",
    "# 2. Serialize an artifact\n",
    "# After this call you will have an `encoder.pt`, `decoder.pt` and `config.json` in the neuron_name folder\n",
    "model_neuron.save_pretrained(neuron_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32d3a0",
   "metadata": {},
   "source": [
    "## 5) A simple test to check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb43f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      "1 Ich bin ein kleiner Frosch\n",
      "2 Ich bin ein kleiner Frosch.\n",
      "3 Ich bin ein kleiner Frosch!\n",
      "4 - Ich bin ein kleiner Frosch.\n"
     ]
    }
   ],
   "source": [
    "infer(model_neuron, tokenizer, text, num_beams, max_encoder_length, max_decoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d766fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
