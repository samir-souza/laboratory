{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c36d0-f910-4288-9527-cb8b75a59c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy HF LLMs to Inferentia2 and SageMaker\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc514194-480e-422c-b9be-c39bf4b48760",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Update SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf072e-8c44-4394-8ca2-229db89915ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dfa12-a2e7-479b-8c28-1fd962668e5f",
   "metadata": {},
   "source": [
    "## 2) Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869de2b-9b9b-4942-8ce6-ce1e51f2580e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable\n",
    "HF_TOKEN=None\n",
    "\n",
    "assert not HF_TOKEN is None, \"Go to your HF account and get an access token. Set HF_TOKEN to your token\"\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f62486-3857-45ad-a21d-f37255ce1833",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1) Build NeuronSDK 2.16 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86178b2-b8cb-41bd-bfcb-674f56584302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"container/training\", exist_ok=True)\n",
    "os.makedirs(\"container/inference\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aac1fa-0fbb-4c12-bcb7-4dc513c8eb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile container/training/Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\n",
    "RUN apt update -y && \\\n",
    "    apt install -y aws-neuronx-collectives aws-neuronx-runtime-lib aws-neuronx-tools && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN pip3 install --extra-index-url https://pip.repos.neuron.amazonaws.com \\\n",
    "    transformers==4.36.2 \\\n",
    "    libneuronxla==0.5.669 \\\n",
    "    torch-neuronx==1.13.1.1.13.0 \\\n",
    "    transformers-neuronx==0.9.474 \\\n",
    "    torch-xla==1.13.1+torchneurond \\\n",
    "    neuronx-cc==2.12.54.0+f631c2365 \\\n",
    "    neuronx-hwm==2.12.0.0+422c9037c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d893f17-afb5-4813-ae5f-84d1a6afbbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sm-docker build container/training \\\n",
    "    --repository pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.16.0-ubuntu20.04 \\\n",
    "    --build-arg REGION=$region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5ce18-541f-43fd-8833-cbc188b31980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile container/inference/Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\n",
    "RUN apt update -y && \\\n",
    "    apt install -y aws-neuronx-collectives aws-neuronx-runtime-lib aws-neuronx-tools && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN pip3 install --extra-index-url https://pip.repos.neuron.amazonaws.com \\\n",
    "    transformers==4.36.2 \\\n",
    "    libneuronxla==0.5.669 \\\n",
    "    torch-neuronx==1.13.1.1.13.0 \\\n",
    "    transformers-neuronx==0.9.474 \\\n",
    "    torch-xla==1.13.1+torchneurond \\\n",
    "    neuronx-cc==2.12.54.0+f631c2365 \\\n",
    "    neuronx-hwm==2.12.0.0+422c9037c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d0e6a-45ed-4fde-a6cf-d224028b19b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sm-docker build container/inference \\\n",
    "    --repository pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.16.0-ubuntu20.04 \\\n",
    "    --build-arg REGION=$region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ad087-6f22-4513-9550-c23911026de7",
   "metadata": {},
   "source": [
    "## 3) Install additional packages before compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1e5d-76d3-4940-aeef-09feede76d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "transformers==4.36.2\n",
    "transformers-neuronx==0.9.474"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eff2d-31a7-49fd-89ab-23a800bb0814",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Create now Python scripts for compiling and deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285bd5-3b17-47f9-96d0-b38deb5c3311",
   "metadata": {},
   "source": [
    "### 4.1) This script will download model weights from HF, split into multiple files and compile the model for a given number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a15a-1a24-4248-b0aa-1e3031000eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES']=os.environ.get('TP_DEGREE', os.environ.get('SM_NUM_NEURONS', '2'))\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--logfile=/dev/null --model-type=transformer\"\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import importlib\n",
    "import traceback\n",
    "import transformers_neuronx\n",
    "\n",
    "from importlib import reload\n",
    "from threading import Thread\n",
    "from huggingface_hub import login\n",
    "from filelock import Timeout, FileLock\n",
    "from transformers_neuronx import constants\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from transformers_neuronx.config import NeuronConfig, QuantizationConfig\n",
    "\n",
    "try:\n",
    "    from ts.protocol.otf_message_handler import send_intermediate_predict_response\n",
    "except ModuleNotFoundError as e:\n",
    "    # this is required only for inference not for training\n",
    "    print(\"Package TS not found. Streaming disabled.\")\n",
    "\n",
    "lock_path='/tmp/new_packages.lock'\n",
    "lock = FileLock(lock_path)\n",
    "\n",
    "def compile_or_load_model(model_dir, model_arch, **kwargs):\n",
    "    '''\n",
    "    If the model artifacts are in the model_dir just load the model,\n",
    "    otherwise, compile it and generate the artifacts.\n",
    "    '''\n",
    "    os.environ['NEURONX_DUMP_TO'] = os.path.join(model_dir, \"neuron_cache\")\n",
    "    print(kwargs)\n",
    "    t=time.time()\n",
    "    print(f\"Loading... Model arch: {model_arch}\")\n",
    "    model_class = f\"transformers_neuronx.{model_arch.lower()}.model.{model_arch.title()}ForSampling\"\n",
    "    importlib.import_module(f\"transformers_neuronx.{model_arch.lower()}.model\")\n",
    "    AutoModelForSampling = eval(model_class)\n",
    "    \n",
    "    model = AutoModelForSampling.from_pretrained(os.path.join(model_dir, \"model-split\"), **kwargs)\n",
    "    model.to_neuron()\n",
    "    print(f\"Elapsed: {time.time()-t}s\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    # this lock is necessary to serialize model loading\n",
    "    # when you have multiple workers trying to load different\n",
    "    # copies using the same hardware\n",
    "    print(\"Waiting for the lock acquire...\")\n",
    "    lock.acquire()\n",
    "    conf = json.load(open(os.path.join(model_dir, \"params.json\"), 'r'))\n",
    "    fields = [\"batch_size\", \"tp_degree\", \"amp\", \"n_positions\", \"gqa\"]\n",
    "    kwargs = {f:conf[f] for f in fields}\n",
    "    \n",
    "    neuron_config = NeuronConfig()\n",
    "    if kwargs['amp'] == 's8':\n",
    "        neuron_config.quant=QuantizationConfig(quant_dtype='s8', dequant_dtype='bf16')\n",
    "        kwargs['amp'] = 'bf16'\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "    \n",
    "    if not kwargs.get('gqa') is None:\n",
    "        neuron_config.group_query_attention = eval(f\"constants.GQA.{kwargs['gqa']}\")\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "\n",
    "    model = compile_or_load_model(model_dir, conf['model_arch'], **kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    streamer = TextIteratorStreamer(tokenizer)\n",
    "    lock.release()\n",
    "    print(\"Lock released\")\n",
    "    return model,tokenizer,streamer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        seq_len = req.get('sequence_length', 2048)\n",
    "        top_k = req.get('top_k', 50)\n",
    "        top_p = req.get('top_p', 1.0)\n",
    "        temperature = req.get('temperature', 1.0)\n",
    "        stream = req.get('stream', False) # enables streaming\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt,seq_len,top_k,top_p,temperature,stream\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json. Expected keys: prompt,optional[sequence_length,top_k,top_p,temperature,stream]\")\n",
    "\n",
    "def predict_fn(input_object, model_tokenizer_streamer, context=None):\n",
    "    model,tokenizer,streamer = model_tokenizer_streamer\n",
    "    prompt,seq_len,top_k,top_p,temperature,stream = input_object\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    if stream:\n",
    "        # stream the tokens/words to the client as soon as they are decoded\n",
    "        def predict(model, input_ids, sequence_length, top_k, top_p, temperature, streamer):\n",
    "            with torch.inference_mode():\n",
    "                generated_sequences = model.sample(input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature, streamer=streamer)\n",
    "        generation_kwargs = dict(model=model, input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature, streamer=streamer)\n",
    "        thread = Thread(target=predict, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "        for part in streamer:\n",
    "            if len(part) == 0: continue\n",
    "            send_intermediate_predict_response([part], context.request_ids, \"Intermediate Prediction success\", 200, context)\n",
    "        thread.join()\n",
    "        # Do not return anything when streaming, otherwise it will kill the worker\n",
    "        # this is a workaround that needs to be handled by the client\n",
    "        raise Warning(\"__END_OF_PREDICTION__\")\n",
    "    else:\n",
    "        # collect all the words/tokens before sending it to the customer\n",
    "        with torch.inference_mode():\n",
    "            generated_sequences = model.sample(input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "            return [tokenizer.decode(s) for s in generated_sequences]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    parser.add_argument(\"--model_arch\", type=str, required=True)\n",
    "    parser.add_argument(\"--hf_access_token\", type=str, default=None)\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "\n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2)\n",
    "    parser.add_argument(\"--n_positions\", type=int, default=2048)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--dtype\", type=str, default='bf16', choices=['s8', 'bf16', 'fp16', 'fp32'])\n",
    "    parser.add_argument(\"--gqa\", type=str, default=None, choices=[i.name for i in constants.GQA])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    model_class = f\"transformers_neuronx.{args.model_arch.lower()}.model.{args.model_arch.title()}ForSampling\"\n",
    "    importlib.import_module(f\"transformers_neuronx.{args.model_arch.lower()}.model\")\n",
    "    AutoModelForSampling = eval(model_class)\n",
    "\n",
    "    if args.hf_access_token:\n",
    "        login(args.hf_access_token)\n",
    "    print(\"Loading model...\")\n",
    "    t=time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_id)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Spliting and saving...\")\n",
    "    t=time.time()\n",
    "    save_pretrained_split(model, os.path.join(args.model_dir, \"model-split\"))\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Saving tokenizer...\")\n",
    "    t=time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Copying inference.py\")\n",
    "    code_path = os.path.join(args.model_dir, \"code\")\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy(\"requirements.txt\", os.path.join(code_path, \"requirements.txt\"))\n",
    "    kwargs = {\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"amp\": args.dtype,\n",
    "        \"tp_degree\": args.tp_degree,\n",
    "        \"n_positions\": args.n_positions,\n",
    "        \"gqa\": args.gqa\n",
    "    }\n",
    "    with open(os.path.join(args.model_dir, \"params.json\"), \"w\") as c:\n",
    "        conf = dict(kwargs)\n",
    "        conf['model_arch'] = args.model_arch\n",
    "        c.write(json.dumps(conf))\n",
    "    \n",
    "    neuron_config = NeuronConfig()\n",
    "    if kwargs['amp'] == 's8':\n",
    "        neuron_config.quant=QuantizationConfig(quant_dtype='s8', dequant_dtype='bf16')\n",
    "        kwargs['amp'] = 'bf16'\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "    \n",
    "    if not kwargs.get('gqa') is None:\n",
    "        neuron_config.group_query_attention = eval(f\"constants.GQA.{kwargs['gqa']}\")\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "    \n",
    "    compile_or_load_model(args.model_dir, args.model_arch, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab929116-c7b6-473c-a21d-f0c6d908fa8b",
   "metadata": {},
   "source": [
    "## 5) SageMaker (training) Job that will download, split and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53d500-169c-49e4-8c87-44313462b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp_degree=2\n",
    "dtype='bf16' # s8, bf16, fp16, fp32\n",
    "batch_size=1\n",
    "sentence_len=1024\n",
    "model_id=\"mistralai/Mistral-7B-Instruct-v0.1\" # \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_arch=\"mistral\" # llama\n",
    "gqa=\"SHARD_OVER_HEADS\" # None\n",
    "assert tp_degree==2 or tp_degree==8, \"2 = cheapest option with higher latency; 8 = more efficient with lower latency;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132be8-9b37-4ec9-8735-94e7d7e6e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type='ml.trn1.32xlarge' if tp_degree > 1 else 'ml.trn1.2xlarge'\n",
    "print(f\"Instance type: {instance_type}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.16.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 128,\n",
    "    hyperparameters={\n",
    "        \"hf_access_token\": HF_TOKEN,\n",
    "        \"model_id\": model_id,\n",
    "        \"model_arch\": model_arch,\n",
    "        \"gqa\": gqa,\n",
    "        \"tp_degree\": tp_degree,\n",
    "        \"n_positions\": sentence_len,\n",
    "        \"dtype\": dtype\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744806-f877-4025-b1c4-c0fdad17105c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes ~21mins on a trn1.32xlarge and ~40mins on a trn1.2xlarge\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8775b-6b72-4fea-a120-9285b93a1676",
   "metadata": {},
   "source": [
    "## 6) Deploy the compiled model to a SageMaker endpoint on inf2\n",
    "Depending on the size of the deployed instance and the number of cores used by the model (**tp_degree**), SageMaker can launch multiple workers. A worker is a standalone Python process that manages one copy of the model. SageMaker puts a load balancer on top of all these processes and distributes the load automatically for your clients. It means that you can increase throughput by launching multiple workers which serve different clients in parallel.\n",
    "\n",
    "For instance. If you set **tp_degree** to 8 and deploy your model to a **ml.inf2.48xlarge**, SageMaker can launch 3 workers with 3 copies of the model. This instance has 24 cores and each model utilizes in this scenario 8 cores. Then, you can have 3 simultaneous clients invoking the endpoint and being served at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29628ca3-fa0e-4fb3-a2ba-c239b41d2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "instance_type_idx=0\n",
    "## Attention: ml.inf2.xlarge doesnt have enough memory to work with llama7b\n",
    "instance_types=['ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_cores=[2,12,24]\n",
    "num_workers=num_cores[instance_type_idx]//tp_degree\n",
    "assert num_workers > 0, f\"Instance {instance_types[instance_type_idx]} doesn't support tp_degree={tp_degree}\"\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.16.0-ubuntu20.04\",\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base(model_arch),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "    },\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325d17-aa4e-4cd3-8190-156984cfdac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    volume_size=128,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191daec-2ab9-42db-b237-76b110b82ea3",
   "metadata": {},
   "source": [
    "## 7) Run a simple test to check the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd892b26-27c5-4e05-8096-f953e1ee4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f196a1-51a8-4edc-a573-aa6aa6fe65a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "def predict(text):\n",
    "    global predictor\n",
    "    t=time.time()\n",
    "    pred = predictor.predict({\"prompt\": text })[0]\n",
    "    print(pred)\n",
    "    elapsed = time.time()-t\n",
    "    answer = re.match(r'^.*\\[\\/INST\\] ?(.*)</s>', pred)[1]\n",
    "    num_words = len(answer.split(' '))\n",
    "    return answer,num_words,elapsed\n",
    "\n",
    "text=\"[INST]Hi, my name is Robot. How are you?[/INST]\"\n",
    "answer,num_words,elapsed=predict(text)\n",
    "print(f\"Num Words: {num_words}, Words/sec: {num_words/elapsed:.04f}, Elapsed time: {elapsed:.04f}s\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70842f3-5020-4885-92a1-ac7b06ec4077",
   "metadata": {},
   "source": [
    "### 7.1) Stream the prediction word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad2888-54ae-41cf-8d11-60d8d22c7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "What is a whole food plant diet? [/INST]\n",
    "\"\"\"\n",
    "body = json.dumps({'prompt': prompt, 'sequence_length': 512, 'temperature': 1.0, 'stream': True}).encode('utf-8')\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    ")\n",
    "eop=False\n",
    "for e in resp['Body']:\n",
    "    tok = e['PayloadPart']['Bytes'].decode('utf-8')\n",
    "    if tok.startswith(\"__END_OF_PREDICTION__\"): eop = True\n",
    "    if not eop: print(tok, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd80235-5803-4de6-a1cb-1114fde3e4fc",
   "metadata": {},
   "source": [
    "### 7.2) Now, launch multiple threads in parallel to simulate concurrent clients\n",
    "Only valid when **num_workers > 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ebea0-23a0-48c1-8442-ee97cd7255e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "with ThreadPool(num_workers) as p:\n",
    "    t=time.time()\n",
    "    resp = p.map(predict, [text] * num_workers)\n",
    "    elapsed=time.time()-t\n",
    "    print(f\"Total elapsed time for {num_workers} workers: {elapsed}\")\n",
    "    \n",
    "    for answer,num_words,elapsed in resp:\n",
    "        print(f\" :: Num Words: {num_words}, Words/sec: {num_words/elapsed:.04f}, Elapsed time: {elapsed:.04f}s\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f65723-ed03-44d6-bded-8df1147d6170",
   "metadata": {},
   "source": [
    "## 8) Cleanup\n",
    "Delete the endpoint to stop paying for the provisioned resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b48f9a-305f-4bd5-bcd7-eef8b474c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
