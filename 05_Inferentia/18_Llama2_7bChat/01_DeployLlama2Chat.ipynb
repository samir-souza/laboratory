{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c36d0-f910-4288-9527-cb8b75a59c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy Llama2 7b Chat HF to Inferentia2 and SageMaker\n",
    "\n",
    "**Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "### Run the following steps to get permission to download LLama2 pre-trained weights from Meta\n",
    "\n",
    "#### Step 1 - HF Account\n",
    "Go to (https://huggingface.co/join) and create a HF account if you don't have one. Log into HF hub after that.\n",
    "\n",
    "#### Step 2 - Create and Access token\n",
    "Follow the instrutions from (https://huggingface.co/docs/hub/security-tokens) and create a new Access token. Copy the token.\n",
    "\n",
    "#### Step 3 - Meta approval to download weights\n",
    "Follow the instructions from (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) to get an approval from Meta for you to download and use the weights. It can take some time. After approved you'll see a message like: **Gated model You have been granted access to this model** at the top of the same page. Now you're ready to download and compile your model to Inferentia2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc514194-480e-422c-b9be-c39bf4b48760",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Update SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf072e-8c44-4394-8ca2-229db89915ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dfa12-a2e7-479b-8c28-1fd962668e5f",
   "metadata": {},
   "source": [
    "## 2) Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869de2b-9b9b-4942-8ce6-ce1e51f2580e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable\n",
    "HF_TOKEN=None\n",
    "\n",
    "assert not HF_TOKEN is None, \"Go to your HF account and get an access token. Set HF_TOKEN to your token\"\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ad087-6f22-4513-9550-c23911026de7",
   "metadata": {},
   "source": [
    "## 3) Install additional packages before compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1e5d-76d3-4940-aeef-09feede76d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "transformers-neuronx==0.6.106"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eff2d-31a7-49fd-89ab-23a800bb0814",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Create now Python scripts for compiling and deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285bd5-3b17-47f9-96d0-b38deb5c3311",
   "metadata": {},
   "source": [
    "### 4.1) This script will download model weights from HF, split into multiple files and compile the model for a given number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a15a-1a24-4248-b0aa-1e3031000eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/split_llama2.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import traceback\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--logfile=/dev/null --model-type=transformer-inference\"\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.    \n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")    \n",
    "    parser.add_argument(\"--hf_access_token\", type=str, required=True)\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    \n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2)\n",
    "    parser.add_argument(\"--n_positions\", type=int, default=1024)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--dtype\", type=str, default='bf16')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    os.environ['NEURONX_DUMP_TO']=os.path.join(args.model_dir, \"neuron_cache\")\n",
    "\n",
    "    login(args.hf_access_token)\n",
    "    print(\"Loading model...\")\n",
    "    t=time.time()\n",
    "    model = LlamaForCausalLM.from_pretrained(args.model_id)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Spliting and saving...\")\n",
    "    t=time.time()\n",
    "    save_pretrained_split(model, os.path.join(args.model_dir, \"llama2-split\"))\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Saving tokenizer...\")\n",
    "    t=time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Copying inference.py\")\n",
    "    code_path = os.path.join(args.model_dir, \"code\")\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "    shutil.copy(\"inference.py\", os.path.join(code_path, \"inference.py\"))\n",
    "\n",
    "    kwargs = {\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"amp\": args.dtype,\n",
    "        \"tp_degree\": args.tp_degree,\n",
    "        \"n_positions\": args.n_positions,\n",
    "        \"unroll\": None\n",
    "    }\n",
    "    print(\"Compiling model...\")\n",
    "    t=time.time()\n",
    "    model = LlamaForSampling.from_pretrained(os.path.join(args.model_dir, \"llama2-split\"), **kwargs)\n",
    "    model.to_neuron()\n",
    "    model._save_compiled_artifacts(os.path.join(args.model_dir, \"artifacts\"))\n",
    "    print(f\"Compilation time: {time.time()-t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26820d-c3f0-41c7-9db6-94b4b0f9dbe9",
   "metadata": {},
   "source": [
    "### 4.2) Script used by SageMaker to load the model and invoke it as an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbb92c-d01a-4025-90bd-06c38e3ca38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/inference.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from filelock import Timeout, FileLock\n",
    "\n",
    "lock_path='/tmp/new_packages.lock'\n",
    "lock = FileLock(lock_path)\n",
    "\n",
    "os.environ['NEURON_RT_NUM_CORES']=os.environ.get('TP_DEGREE', 8)\n",
    "# if you're using NeuronSDK version 2.14+ use: --model-type=transformer\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--logfile=/dev/null --model-type=transformer-inference\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    os.environ['NEURONX_DUMP_TO'] = os.path.join(model_dir, \"neuron_cache\")\n",
    "    batch_size=int(os.environ.get('BATCH_SIZE', 1))\n",
    "    tp_degree=int(os.environ.get('TP_DEGREE', 8))\n",
    "    dtype=os.environ.get('DTYPE', 'bf16')\n",
    "\n",
    "    print(\"Waiting for the lock acquire...\")    \n",
    "    lock.acquire()\n",
    "    t=time.time()\n",
    "    print(\"Loading model...\")\n",
    "    model = LlamaForSampling.from_pretrained(os.path.join(model_dir, \"llama2-split\"), batch_size=batch_size, tp_degree=tp_degree, amp=dtype)\n",
    "    neuron_program_path = os.path.join(model_dir, \"artifacts\", \"neuron-program.pkl\")\n",
    "    if os.path.isfile(neuron_program_path):\n",
    "        print(\"Neuron program found. Loading\")\n",
    "        model._load_compiled_artifacts(os.path.join(model_dir, \"artifacts\"))\n",
    "    model.to_neuron()\n",
    "    print(f\"Model loaded. Elapsed: {time.time()-t}s\")\n",
    "    lock.release()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model,tokenizer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        seq_len = req.get('sequence_length', 2048)\n",
    "        top_k = req.get('top_k', 50)\n",
    "        top_p = req.get('top_p', 1.0)\n",
    "        temperature = req.get('temperature', 1.0)\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt,seq_len,top_k,top_p,temperature\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json. Expected keys: prompt,optional[sequence_length,top_k,top_p,temperature]\")\n",
    "\n",
    "def predict_fn(input_object, model_tokenizer, context=None):\n",
    "    model,tokenizer = model_tokenizer\n",
    "    prompt,seq_len,top_k,top_p,temperature = input_object\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # run inference with top-k sampling\n",
    "    t=time.time()\n",
    "    with torch.inference_mode():\n",
    "        generated_sequences = model.sample(input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "        out = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "        print(f\"Pred. elapsed: {time.time()-t}s\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab929116-c7b6-473c-a21d-f0c6d908fa8b",
   "metadata": {},
   "source": [
    "## 5) SageMaker (training) Job that will download, split and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53d500-169c-49e4-8c87-44313462b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp_degree=2\n",
    "dtype='bf16'\n",
    "batch_size=1\n",
    "sentence_len=2048\n",
    "assert tp_degree==2 or tp_degree==8, \"2 = cheapest option with higher latency; 8 = more efficient with lower latency;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132be8-9b37-4ec9-8735-94e7d7e6e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type='ml.trn1.32xlarge' if tp_degree > 2 else 'ml.trn1.2xlarge'\n",
    "print(f\"Instance type: {instance_type}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"split_llama2.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.13.2-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 128,\n",
    "    hyperparameters={\n",
    "        \"hf_access_token\": HF_TOKEN,\n",
    "        \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"tp_degree\": tp_degree,\n",
    "        \"n_positions\": sentence_len\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744806-f877-4025-b1c4-c0fdad17105c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes ~21mins on a trn1.32xlarge and ~40mins on a trn1.2xlarge\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8775b-6b72-4fea-a120-9285b93a1676",
   "metadata": {},
   "source": [
    "## 6) Deploy the compiled model to a SageMaker endpoint on inf2\n",
    "Depending on the size of the deployed instance and the number of cores used by the model (**tp_degree**), SageMaker can launch multiple workers. A worker is a standalone Python process that manages one copy of the model. SageMaker puts a load balancer on top of all these processes and distributes the load automatically for your clients. It means that you can increase throughput by launching multiple workers which serve different clients in parallel.\n",
    "\n",
    "For instance. If you set **tp_degree** to 8 and deploy your model to a **ml.inf2.48xlarge**, SageMaker can launch 3 workers with 3 copies of the model. This instance has 24 cores and each model utilizes in this scenario 8 cores. Then, you can have 3 simultaneous clients invoking the endpoint and being served at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29628ca3-fa0e-4fb3-a2ba-c239b41d2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "instance_type_idx=0\n",
    "## Attention: ml.inf2.xlarge doesnt have enough memory to work with llama7b\n",
    "instance_types=['ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_cores=[2,12,24]\n",
    "num_workers=num_cores[instance_type_idx]//tp_degree\n",
    "assert num_workers > 0, f\"Instance {instance_types[instance_type_idx]} doesn't support tp_degree={tp_degree}\"\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.13.2-ubuntu20.04\",\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('llama2-7b-chat'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "        'TP_DEGREE': str(tp_degree),\n",
    "        'BATCH_SIZE': str(batch_size),\n",
    "        'DTYPE': dtype\n",
    "    },\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325d17-aa4e-4cd3-8190-156984cfdac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191daec-2ab9-42db-b237-76b110b82ea3",
   "metadata": {},
   "source": [
    "## 7) Run a simple test to check the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "dd892b26-27c5-4e05-8096-f953e1ee4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "12f196a1-51a8-4edc-a573-aa6aa6fe65a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Words: 43, Words/sec: 23.8324, Elapsed time: 1.8043s\n",
      "Answer: Hello Adam! I'm just an AI, so I don't have feelings or emotions like humans do, but I'm here to help you in any way I can. How can I assist you today? Is there anything you'd like to chat about or ask?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "text=\"[INST]Hi, my name is Adam. How are you?[/INST]\"\n",
    "t=time.time()\n",
    "pred = predictor.predict({\"prompt\": text })[0]\n",
    "elapsed = time.time()-t\n",
    "answer = re.match(r'^.*\\[\\/INST\\] +(.*)</s>', pred)[1]\n",
    "num_words = len(answer.split(' '))\n",
    "print(f\"Num Words: {num_words}, Words/sec: {num_words/elapsed:.04f}, Elapsed time: {elapsed:.04f}s\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6a7ebea0-23a0-48c1-8442-ee97cd7255e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I'm just an AI, I don't have feelings or emotions, but I'm here to help you with any questions or tasks you may have. How can I assist you today?</s>\"], ['<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I\\'m just an AI, I don\\'t have feelings or emotions like a human, so I don\\'t have a personal experience of being \"good\" or \"bad.\" However, I\\'m here to help you with any questions or tasks you may have, so please feel free to ask me anything!</s>'], [\"<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I'm just an AI, I don't have feelings or emotions like humans do, so I don't feel anything in response to your greeting. However, I'm here to help you with any questions or tasks you may have, so feel free to ask me anything!</s>\"], [\"<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I'm just an AI, I don't have feelings or emotions like humans do, so I can't feel or respond to emotions. However, I'm here to help you with any questions or tasks you may have! How can I assist you today?</s>\"], [\"<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I'm just an AI, I don't have emotions or feelings like a human would, so I don't have a personal experience of well-being. However, I'm here to help you with any questions or tasks you may have, so feel free to ask me anything!</s>\"], [\"<s> [INST]Hi, my name is Adam. How are you?[/INST]  Hello Adam! I'm just an AI, I don't have feelings or emotions, but I'm here to help you in any way I can. How can I assist you today? Is there something you'd like to chat about or ask? ðŸ˜Š</s>\"]]\n",
      "Elapsed time: 4.121546030044556\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "with ThreadPool(num_workers) as p:\n",
    "    t=time.time()\n",
    "    print(p.map(predictor.predict, [{\"prompt\": text}] * num_workers))\n",
    "    print(f\"Elapsed time: {time.time()-t}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
