{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c36d0-f910-4288-9527-cb8b75a59c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy Llama2 7b Chat HF to Inferentia2 and SageMaker\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "### Run the following steps to get permission to download LLama2 pre-trained weights from Meta\n",
    "\n",
    "#### Step 1 - HF Account\n",
    "Go to (https://huggingface.co/join) and create a HF account if you don't have one. Log into HF hub after that.\n",
    "\n",
    "#### Step 2 - Create and Access token\n",
    "Follow the instrutions from (https://huggingface.co/docs/hub/security-tokens) and create a new Access token. Copy the token.\n",
    "\n",
    "#### Step 3 - Meta approval to download weights\n",
    "Follow the instructions from (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) to get an approval from Meta for you to download and use the weights. It can take some time. After approved you'll see a message like: **Gated model You have been granted access to this model** at the top of the same page. Now you're ready to download and compile your model to Inferentia2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc514194-480e-422c-b9be-c39bf4b48760",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Update SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf072e-8c44-4394-8ca2-229db89915ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dfa12-a2e7-479b-8c28-1fd962668e5f",
   "metadata": {},
   "source": [
    "## 2) Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869de2b-9b9b-4942-8ce6-ce1e51f2580e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable\n",
    "HF_TOKEN=None\n",
    "\n",
    "assert not HF_TOKEN is None, \"Go to your HF account and get an access token. Set HF_TOKEN to your token\"\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ad087-6f22-4513-9550-c23911026de7",
   "metadata": {},
   "source": [
    "## 3) Install additional packages before compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1e5d-76d3-4940-aeef-09feede76d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "transformers==4.33.1\n",
    "transformers-neuronx==0.8.268"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eff2d-31a7-49fd-89ab-23a800bb0814",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Create now Python scripts for compiling and deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285bd5-3b17-47f9-96d0-b38deb5c3311",
   "metadata": {},
   "source": [
    "### 4.1) This script will download model weights from HF, split into multiple files and compile the model for a given number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a15a-1a24-4248-b0aa-1e3031000eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES']=os.environ.get('TP_DEGREE', os.environ.get('SM_NUM_NEURONS', '2'))\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--logfile=/dev/null --model-type=transformer\"\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import traceback\n",
    "\n",
    "from threading import Thread\n",
    "from huggingface_hub import login\n",
    "from filelock import Timeout, FileLock\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from transformers_neuronx.config import NeuronConfig, QuantizationConfig\n",
    "\n",
    "try:\n",
    "    from ts.protocol.otf_message_handler import send_intermediate_predict_response\n",
    "except ModuleNotFoundError as e:\n",
    "    # this is required only for inference not for training\n",
    "    print(e)\n",
    "\n",
    "lock_path='/tmp/new_packages.lock'\n",
    "lock = FileLock(lock_path)\n",
    "\n",
    "def compile_or_load_model(model_dir, **kwargs):\n",
    "    '''\n",
    "    If the model artifacts are in the model_dir just load the model,\n",
    "    otherwise, compile it and generate the artifacts.\n",
    "    '''\n",
    "    os.environ['NEURONX_DUMP_TO'] = os.path.join(model_dir, \"neuron_cache\")\n",
    "    neuron_artifacts_path = os.path.join(model_dir, \"artifacts\")\n",
    "    has_artifacts = os.path.isdir(neuron_artifacts_path)\n",
    "    print(kwargs)\n",
    "    t=time.time()\n",
    "    print(\"Loading...\" if has_artifacts else \"Compiling...\")\n",
    "    model = LlamaForSampling.from_pretrained(os.path.join(model_dir, \"llama2-split\"), **kwargs)\n",
    "    if has_artifacts:\n",
    "        print(\"Neuron program found. Loading\")\n",
    "        model._load_compiled_artifacts(neuron_artifacts_path)\n",
    "    model.to_neuron()\n",
    "    if not has_artifacts:\n",
    "        model._save_compiled_artifacts(neuron_artifacts_path)\n",
    "    print(f\"Elapsed: {time.time()-t}s\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    # this lock is necessary to serialize model loading\n",
    "    # when you have multiple workers trying to load different\n",
    "    # copies using the same hardware\n",
    "    print(\"Waiting for the lock acquire...\")\n",
    "    lock.acquire()\n",
    "    kwargs = {\n",
    "        \"batch_size\": int(os.environ.get('BATCH_SIZE', 1)),\n",
    "        \"tp_degree\": int(os.environ.get('TP_DEGREE', 8)),\n",
    "        \"amp\": os.environ.get('DTYPE', 'bf16'),\n",
    "        \"n_positions\": int(os.environ.get('SEQ_LEN', 2048)),\n",
    "    }\n",
    "    if kwargs['amp'] == 's8':\n",
    "        neuron_config = NeuronConfig(\n",
    "            quant=QuantizationConfig(quant_dtype='s8', dequant_dtype='bf16'),\n",
    "        )\n",
    "        kwargs['amp'] = 'bf16'\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "\n",
    "    model = compile_or_load_model(model_dir, **kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    streamer = TextIteratorStreamer(tokenizer)\n",
    "    lock.release()\n",
    "    print(\"Lock released\")\n",
    "    return model,tokenizer,streamer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        seq_len = req.get('sequence_length', 2048)\n",
    "        top_k = req.get('top_k', 50)\n",
    "        top_p = req.get('top_p', 1.0)\n",
    "        temperature = req.get('temperature', 1.0)\n",
    "        stream = req.get('stream', False) # enables streaming\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt,seq_len,top_k,top_p,temperature,stream\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json. Expected keys: prompt,optional[sequence_length,top_k,top_p,temperature,stream]\")\n",
    "\n",
    "def predict_fn(input_object, model_tokenizer_streamer, context=None):\n",
    "    model,tokenizer,streamer = model_tokenizer_streamer\n",
    "    prompt,seq_len,top_k,top_p,temperature,stream = input_object\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "    if stream:\n",
    "        # stream the tokens/words to the client as soon as they are decoded\n",
    "        def predict(model, input_ids, sequence_length, top_k, top_p, temperature, streamer):\n",
    "            with torch.inference_mode():\n",
    "                generated_sequences = model.sample(input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature, streamer=streamer)\n",
    "        generation_kwargs = dict(model=model, input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature, streamer=streamer)\n",
    "        thread = Thread(target=predict, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "        for part in streamer:\n",
    "            if len(part) == 0: continue\n",
    "            send_intermediate_predict_response([part], context.request_ids, \"Intermediate Prediction success\", 200, context)\n",
    "        thread.join()\n",
    "        # Do not return anything when streaming, otherwise it will kill the worker\n",
    "        # this is a workaround that needs to be handled by the client\n",
    "        raise Warning(\"__END_OF_PREDICTION__\")\n",
    "    else:\n",
    "        # collect all the words/tokens before sending it to the customer\n",
    "        with torch.inference_mode():\n",
    "            generated_sequences = model.sample(input_ids=input_ids, sequence_length=seq_len, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "            return [tokenizer.decode(s) for s in generated_sequences]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    parser.add_argument(\"--hf_access_token\", type=str, required=True)\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "\n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2)\n",
    "    parser.add_argument(\"--n_positions\", type=int, default=2048)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--dtype\", type=str, default='bf16', choices=['s8', 'bf16', 'fp16', 'fp32'])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    login(args.hf_access_token)\n",
    "    print(\"Loading model...\")\n",
    "    t=time.time()\n",
    "    model = LlamaForCausalLM.from_pretrained(args.model_id)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Spliting and saving...\")\n",
    "    t=time.time()\n",
    "    save_pretrained_split(model, os.path.join(args.model_dir, \"llama2-split\"))\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Saving tokenizer...\")\n",
    "    t=time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "    print(\"Copying inference.py\")\n",
    "    code_path = os.path.join(args.model_dir, \"code\")\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy(\"requirements.txt\", os.path.join(code_path, \"requirements.txt\"))\n",
    "\n",
    "    kwargs = {\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"amp\": args.dtype,\n",
    "        \"tp_degree\": args.tp_degree,\n",
    "        \"n_positions\": args.n_positions,\n",
    "    }\n",
    "    if args.dtype == 's8':\n",
    "        neuron_config = NeuronConfig(\n",
    "            quant=QuantizationConfig(quant_dtype='s8', dequant_dtype='bf16'),\n",
    "        )\n",
    "        kwargs['amp'] = 'bf16'\n",
    "        kwargs['neuron_config'] = neuron_config\n",
    "    compile_or_load_model(args.model_dir, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab929116-c7b6-473c-a21d-f0c6d908fa8b",
   "metadata": {},
   "source": [
    "## 5) SageMaker (training) Job that will download, split and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53d500-169c-49e4-8c87-44313462b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp_degree=2\n",
    "dtype='bf16' # s8, bf16, fp16, fp32\n",
    "batch_size=1\n",
    "sentence_len=4096\n",
    "assert tp_degree==2 or tp_degree==8, \"2 = cheapest option with higher latency; 8 = more efficient with lower latency;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132be8-9b37-4ec9-8735-94e7d7e6e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type='ml.trn1.32xlarge' if tp_degree > 1 else 'ml.trn1.2xlarge'\n",
    "print(f\"Instance type: {instance_type}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 128,\n",
    "    hyperparameters={\n",
    "        \"hf_access_token\": HF_TOKEN,\n",
    "        \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"tp_degree\": tp_degree,\n",
    "        \"n_positions\": sentence_len,\n",
    "        \"dtype\": dtype\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744806-f877-4025-b1c4-c0fdad17105c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes ~21mins on a trn1.32xlarge and ~40mins on a trn1.2xlarge\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8775b-6b72-4fea-a120-9285b93a1676",
   "metadata": {},
   "source": [
    "## 6) Deploy the compiled model to a SageMaker endpoint on inf2\n",
    "Depending on the size of the deployed instance and the number of cores used by the model (**tp_degree**), SageMaker can launch multiple workers. A worker is a standalone Python process that manages one copy of the model. SageMaker puts a load balancer on top of all these processes and distributes the load automatically for your clients. It means that you can increase throughput by launching multiple workers which serve different clients in parallel.\n",
    "\n",
    "For instance. If you set **tp_degree** to 8 and deploy your model to a **ml.inf2.48xlarge**, SageMaker can launch 3 workers with 3 copies of the model. This instance has 24 cores and each model utilizes in this scenario 8 cores. Then, you can have 3 simultaneous clients invoking the endpoint and being served at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29628ca3-fa0e-4fb3-a2ba-c239b41d2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "instance_type_idx=1\n",
    "## Attention: ml.inf2.xlarge doesnt have enough memory to work with llama7b\n",
    "instance_types=['ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_cores=[2,12,24]\n",
    "num_workers=num_cores[instance_type_idx]//tp_degree\n",
    "assert num_workers > 0, f\"Instance {instance_types[instance_type_idx]} doesn't support tp_degree={tp_degree}\"\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\",\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('llama2-7b-chat'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "        'TP_DEGREE': str(tp_degree),\n",
    "        'BATCH_SIZE': str(batch_size),\n",
    "        'DTYPE': dtype,\n",
    "        'SEQ_LEN': str(sentence_len)\n",
    "    },\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325d17-aa4e-4cd3-8190-156984cfdac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    volume_size=128,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191daec-2ab9-42db-b237-76b110b82ea3",
   "metadata": {},
   "source": [
    "## 7) Run a simple test to check the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd892b26-27c5-4e05-8096-f953e1ee4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12f196a1-51a8-4edc-a573-aa6aa6fe65a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Words: 36, Words/sec: 15.1571, Elapsed time: 2.3751s\n",
      "Answer: Hello Llama2! *exaggerated llama noise* I'm great, thanks for asking! *giggles* It's always nice to meet a fellow llama enthusiast *winks*. How about you? What brings you to this neck of the woods? *nuzzles virtual ground*\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "def predict(text):\n",
    "    global predictor\n",
    "    t=time.time()\n",
    "    pred = predictor.predict({\"prompt\": text })[0]\n",
    "    elapsed = time.time()-t\n",
    "    answer = re.match(r'^.*\\[\\/INST\\] +(.*)</s>', pred)[1]\n",
    "    num_words = len(answer.split(' '))\n",
    "    return answer,num_words,elapsed\n",
    "\n",
    "text=\"[INST]Hi, my name is Llama2. How are you?[/INST]\"\n",
    "answer,num_words,elapsed=predict(text)\n",
    "print(f\"Num Words: {num_words}, Words/sec: {num_words/elapsed:.04f}, Elapsed time: {elapsed:.04f}s\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70842f3-5020-4885-92a1-ac7b06ec4077",
   "metadata": {},
   "source": [
    "### 7.1) Stream the prediction word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6ad2888-54ae-41cf-8d11-60d8d22c7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Good morning! I'm doing well, thank you for asking. How about you, Llama2? Is there anything I can help you with or do you have any questions? I'm here to assist you in any way I can, while being safe and social unbiased.</s>"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Good morning. My name is Llama2. How are you? [/INST]\n",
    "\"\"\"\n",
    "body = json.dumps({'prompt': prompt, 'sequence_length': 512, 'temperature': 1.0, 'stream': True}).encode('utf-8')\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    ")\n",
    "eop=False\n",
    "for e in resp['Body']:\n",
    "    tok = e['PayloadPart']['Bytes'].decode('utf-8')\n",
    "    if tok.startswith(\"__END_OF_PREDICTION__\"): eop = True\n",
    "    if not eop: print(tok, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd80235-5803-4de6-a1cb-1114fde3e4fc",
   "metadata": {},
   "source": [
    "### 7.2) Now, launch multiple threads in parallel to simulate concurrent clients\n",
    "Only valid when **num_workers > 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a7ebea0-23a0-48c1-8442-ee97cd7255e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time for 6 workers: 4.499519348144531\n",
      " :: Num Words: 27, Words/sec: 14.9656, Elapsed time: 1.8041s\n",
      "Answer: Hello Llama2! *adjusts glasses* I'm doing well, thank you for asking! It's always nice to meet a fellow llama enthusiast. How are you feeling today? *nerdily smiles*\n",
      " :: Num Words: 53, Words/sec: 16.3852, Elapsed time: 3.2346s\n",
      "Answer: Hello Llama2! 😊 I'm just an AI, I don't have feelings or emotions like humans do, so I'm not capable of feeling or expressing emotions like \"hello\" or \"how are you?\" However, I'm here to help you with any questions or tasks you may have, so feel free to ask me anything! 🤖\n",
      " :: Num Words: 28, Words/sec: 13.9892, Elapsed time: 2.0015s\n",
      "Answer: Hello Llama2! *twinkle* I'm doing great, thanks for asking! It's always nice to meet a fellow llama fan *chuckles* Is there anything interesting happening in your world? 😊\n",
      " :: Num Words: 56, Words/sec: 15.3175, Elapsed time: 3.6560s\n",
      "Answer: Hello Llama2! *adjusts glasses* I'm doing well, thank you for asking! It's always nice to meet a fellow llama enthusiast. *chuckles* By the way, I'm just an AI, I don't have feelings like humans do, but I'm here to help you with any questions or topics you'd like to discuss. So, what's on your mind? 😊\n",
      " :: Num Words: 32, Words/sec: 13.8715, Elapsed time: 2.3069s\n",
      "Answer: Hello there, Llama2! *ahem* I'm doing well, thanks for asking! *adjusts monocle* It's always a pleasure to meet a fellow llama. How are you faring in this neck of the woods? *winks*\n",
      " :: Num Words: 37, Words/sec: 8.2438, Elapsed time: 4.4882s\n",
      "Answer: Hello Llama2! *bursts into a fit of giggles* Oh, I'm just fabulous, darling! *adjusts monocle* It's always a joy to meet a fellow llama. *winks* How about you, my dear? Having a lovely day? *adjusts monocle again*\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "with ThreadPool(num_workers) as p:\n",
    "    t=time.time()\n",
    "    resp = p.map(predict, [text] * num_workers)\n",
    "    elapsed=time.time()-t\n",
    "    print(f\"Total elapsed time for {num_workers} workers: {elapsed}\")\n",
    "    \n",
    "    for answer,num_words,elapsed in resp:\n",
    "        print(f\" :: Num Words: {num_words}, Words/sec: {num_words/elapsed:.04f}, Elapsed time: {elapsed:.04f}s\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d3b75-ef94-42a1-9d66-0dfb059d4b88",
   "metadata": {},
   "source": [
    "### 7.3) Use this model as the brain of a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eed79-3c86-4c21-9c1d-55a5bc6f9ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the name of the endpoint to use in the second notebook\n",
    "with open('endpoint_name.txt', 'w') as f:\n",
    "    f.write(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3c3c7-b7d7-498f-baf4-34918cf3ac05",
   "metadata": {},
   "source": [
    "Open this notebook to build a smart agent with the deployed model: [02_Llama2SmartAgent](02_Llama2SmartAgent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f65723-ed03-44d6-bded-8df1147d6170",
   "metadata": {},
   "source": [
    "## 8) Cleanup\n",
    "Delete the endpoint to stop paying for the provisioned resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b48f9a-305f-4bd5-bcd7-eef8b474c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
