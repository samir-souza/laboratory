{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8b548c-b1ad-4aae-ab2f-e42218f9807c",
   "metadata": {},
   "source": [
    "# State tensor tracing/inference\n",
    "\n",
    "There are some scenarios your model needs to return large tensors as output and you need to pass these outputs again to the model to continue the inference. A good example of that is an Encoder. In a scenario like that, moving tensors from HBM to Host Memory and vice-versa is slow and sometimes you will not enough memory to allocate everything.\n",
    "\n",
    "State tensors allow you to keep a memory space reserved for a set of tensors. To update their values you use an API provided by the traced model, like you see below.\n",
    "\n",
    "You can see a complete example of how state tensors are important for decoder models [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/torch-neuronx/t5-inference-tutorial.html). As you'll notice, the example is too complex for someone to understand how to use this mechanism. That's why you have this notebook.\n",
    "\n",
    "Also, you can take a [look at the tracing api](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#torch_neuronx.trace), where you see the input parameter **input_output_aliases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8702d1-3e62-4ec6-bff0-58701a2373ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(MLP, self).__init__()\n",
    "      ## Dummy linear layer\n",
    "      self.fc = nn.Linear(2, 2)\n",
    "      ## Special set of tensors that will transformed into state tensors\n",
    "      self.cache = nn.ParameterList(\n",
    "        [\n",
    "            nn.Parameter(torch.zeros((2,2), dtype=torch.float32), requires_grad=False)\n",
    "        ]\n",
    "    ) \n",
    "  def forward(self, x):\n",
    "      x = F.relu(self.fc(x))\n",
    "      for p in self.cache:\n",
    "          x = p * x\n",
    "      cache = [p + 1 for p in self.cache]\n",
    "      return x, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b1dde-b194-40d1-ad9f-e074fd6174f1",
   "metadata": {},
   "source": [
    "Please note that there is no input parameter for the cache in the forward. I'm returning a 2nd output with a tensor that uses the ParameterList based cache.  \n",
    "### Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d643f-0c79-4a57-8c4e-0d8775bb4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP()\n",
    "x = torch.rand((2), dtype=torch.float32)\n",
    "y,cache = m(x)\n",
    "y,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5129fd5-78ab-47ad-a638-3cf8d970a93d",
   "metadata": {},
   "source": [
    "### Trace\n",
    "\n",
    "idx=1 because x is the output element 0, so our cache will be 1, given we have only 1 element in the cache. However if you have a list of tensors, like you do when build a kv_cache, you'll have a list and each element of the list will count as an additional parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e435a-3138-4d04-a6d8-3f9e38e0f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_neuronx\n",
    "\n",
    "idx=1 # \n",
    "aliases = {c:idx+i for i,c in enumerate(m.cache)}\n",
    "neuron_m = torch_neuronx.trace(m, x, input_output_aliases=aliases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a36792-9aa4-41df-b44e-cf8e2973bd14",
   "metadata": {},
   "source": [
    "### Warm up neuron model\n",
    "\n",
    "you should see something like this as the output:\n",
    "```\n",
    "(tensor([[0., 0.],\n",
    "         [0., 0.]]),\n",
    " [tensor([[1., 1.],\n",
    "          [1., 1.]])])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac26f4-9b6c-4fcc-a177-359cf7df8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,cache = neuron_m(x)\n",
    "print(y,cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44153d42-65b6-40dc-b7fd-a6c9656c07b9",
   "metadata": {},
   "source": [
    "### Update state tensor\n",
    "Now let's update the state tensors with a new value of the cache, simulating we're getting the output of the previous invocation and setting new values for the cache:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713befe-2695-4e49-872c-132f6b143933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,p in zip(cache, neuron_m.parameters()):\n",
    "    p.copy_(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc3364-4e52-4218-ac08-f21d407a0899",
   "metadata": {},
   "source": [
    "The command above is how you copy new values to the state tensors without having to duplicate the amount of mem on host.\n",
    "\n",
    "### Now let's invoke the model again:\n",
    "\n",
    "You'll see something like:\n",
    "```\n",
    "tensor([[0.8259, 0.0000],\n",
    "        [0.8259, 0.0000]]) [tensor([[2., 2.],\n",
    "        [2., 2.]])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca19f79-ecb4-410b-afa3-2eca0b634c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,cache = neuron_m(x)\n",
    "print(y,cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
