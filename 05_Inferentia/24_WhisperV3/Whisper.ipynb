{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd387845-cc83-4b60-8ca0-53c26fba0531",
   "metadata": {},
   "source": [
    "# Whisper for Inferentia2\n",
    "\n",
    "This sample shows how to compile & run Whisper models (different sizes) on Inferentia2. It makes use of the HF weights:  \n",
    "  - Tiny: https://huggingface.co/openai/whisper-tiny\n",
    "  - Small: https://huggingface.co/openai/whisper-small\n",
    "  - Medium: https://huggingface.co/openai/whisper-medium\n",
    "  - Large-v3: https://huggingface.co/openai/whisper-large-v3\n",
    "\n",
    "Given the largest model has only 1.5B params, it fits into just 1 core when quantized to bf16. Also, this model is an encoder-decoder, so the strategy is to compile both components individually and then put them back into the original model structure. After that, both encoder and decoder will be accelerated on inf2.\n",
    "\n",
    "You can use the smallest instance for this experiment: inf2.xlarge, but to achieve a higher througput by launching multiple copies of the model to serve clients in parallel, it is recommended to use a larger instance like ml.inf2.24xlarge or trn1.32xlarge.\n",
    "\n",
    "Follow the [instructions from this page to setup the environment.](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx) It is recommended the usage of the following container (DLC) to run your experiments: **Deep Learning Container**: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.19.1-ubuntu20.04\n",
    "\n",
    "This guarantees you'll be using the exact same libraries of this experimentation.\n",
    "\n",
    "Also, make sure you install the following additional libraries in your environment. Pay attention to the transformers version, newer versions might not work.\n",
    "\n",
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `transformers==4.36.2`\n",
    "- `soundfile==0.12.1`\n",
    "- `datasets==2.18.0`\n",
    "- `librosa==0.10.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5448bf-dec2-493c-97f2-9acff99ff7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers==4.36.2 datasets==2.18.0 soundfile==0.12.1 librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2beede5-4669-4aa7-a0bd-98d81823a711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES']='2'\n",
    "import types\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "# please, start by selecting the desired model size\n",
    "suffix=\"tiny\"\n",
    "#suffix=\"small\"\n",
    "#suffix=\"medium\"\n",
    "#suffix=\"large-v3\"\n",
    "model_id=f\"openai/whisper-{suffix}\"\n",
    "\n",
    "# this will load the tokenizer + two copies of the model. cpu_model will be used later for results comparison\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "cpu_model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "\n",
    "# Load a sample from the dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# sample #3 is ~9.9seconds and produces 33 output tokens + pad token\n",
    "sample = dataset[3][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "# output_attentions is required if you want to return word timestamps\n",
    "# if you don't need timestamps, just set this to False and get some better latency\n",
    "output_attentions=False\n",
    "\n",
    "batch_size=1\n",
    "# this is the maximum number of tokens the model will be able to decode\n",
    "# for the sample #3 we selected above, this is enough. If you're planning to \n",
    "# process larger samples, you need to adjust it accordinly.\n",
    "max_dec_len = 64\n",
    "# num_mel_bins,d_model --> these parameters where copied from model.conf (found on HF repo)\n",
    "# we need them to correctly generate dummy inputs during compilation\n",
    "dim_enc=model.config.num_mel_bins\n",
    "dim_dec=model.config.d_model\n",
    "print(f'Dim enc: {dim_enc}; Dim dec: {dim_dec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27073a8f-6efd-49a6-a857-386762add8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions,BaseModelOutput\n",
    "\n",
    "# Now we need to simplify both encoding & decoding forward methods to make them \n",
    "# compilable. Please notice that these methods overwrite the original ones, but\n",
    "# keeps retro-compatibility. Also, we'll use use a new variable \"forward_neuron\"\n",
    "# to invoke the model on inf2\n",
    "def enc_f(self, input_features, attention_mask, **kwargs):\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(input_features, attention_mask)\n",
    "    else:\n",
    "        out = self.forward_(input_features, attention_mask, return_dict=True)\n",
    "    return BaseModelOutput(**out)\n",
    "\n",
    "def dec_f(self, input_ids, attention_mask=None, encoder_hidden_states=None, **kwargs):\n",
    "    out = None        \n",
    "    if not attention_mask is None and encoder_hidden_states is None:\n",
    "        # this is a workaround to align the input parameters for NeuronSDK tracer\n",
    "        # None values are not allowed during compilation\n",
    "        encoder_hidden_states, attention_mask = attention_mask,encoder_hidden_states\n",
    "    inp = [input_ids, encoder_hidden_states]\n",
    "    \n",
    "    # pad the input to max_dec_len\n",
    "    if inp[0].shape[1] > self.max_length:\n",
    "        raise Exception(f\"The decoded sequence is not supported. Max: {self.max_length}\")\n",
    "    pad_size = torch.as_tensor(self.max_length - inp[0].shape[1])\n",
    "    inp[0] = F.pad(inp[0], (0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(*inp)\n",
    "    else:\n",
    "        # output_attentions is required if you want timestamps\n",
    "        out = self.forward_(input_ids=inp[0], encoder_hidden_states=inp[1], return_dict=True, use_cache=False, output_attentions=output_attentions)\n",
    "    # unpad the output\n",
    "    out['last_hidden_state'] = out['last_hidden_state'][:, :input_ids.shape[1], :]\n",
    "    # neuron compiler doesn't like tuples as values of dicts, so we stack them into tensors\n",
    "    # also, we need to average axis=2 given we're not using cache (use_cache=False)\n",
    "    # that way, to avoid an issue with the pipeline we change the shape from:\n",
    "    #  bs,num selected,num_tokens,1500 --> bs,1,num_tokens,1500\n",
    "    # I suspect there is a bug in the HF pipeline code that doesn't support use_cache=False for\n",
    "    # word timestamps, that's why we need that.\n",
    "    if not out.get('attentions') is None:\n",
    "        out['attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :input_ids.shape[1]], axis=2, keepdim=True) for o in out['attentions']])\n",
    "    if not out.get('cross_attentions') is None:\n",
    "        out['cross_attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :], axis=2, keepdim=True) for o in out['cross_attentions']])\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(**out)\n",
    "\n",
    "if not hasattr(model.model.encoder, 'forward_'): model.model.encoder.forward_ = model.model.encoder.forward\n",
    "if not hasattr(model.model.decoder, 'forward_'): model.model.decoder.forward_ = model.model.decoder.forward\n",
    "\n",
    "model.model.encoder.forward = types.MethodType(enc_f, model.model.encoder)\n",
    "model.model.decoder.forward = types.MethodType(dec_f, model.model.decoder)\n",
    "\n",
    "model.model.decoder.max_length = max_dec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd76350-916d-4747-8482-89636940ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346a219-4e02-40f9-8a57-a7c8752aab30",
   "metadata": {},
   "source": [
    "## Trace Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ec9a6-effd-4fae-acf4-1ce1a4109ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_neuron_encoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, dim_enc, 3000], dtype=torch.float32), torch.zeros([1, dim_enc], dtype=torch.int64))\n",
    "    if hasattr(model.model.encoder, 'forward_neuron'): del model.model.encoder.forward_neuron\n",
    "    neuron_encoder = torch_neuronx.trace(\n",
    "        model.model.encoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16',\n",
    "        compiler_workdir='./enc_dir',      \n",
    "        inline_weights_to_neff=False)\n",
    "    neuron_encoder.save(model_filename)\n",
    "    model.model.encoder.forward_neuron = neuron_encoder\n",
    "else:\n",
    "    model.model.encoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a8e74-7971-4f8e-b60a-84aed9bdd186",
   "metadata": {},
   "source": [
    "## Trace decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea4f30-f1a2-47f6-9583-1eb7d26ae882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_{max_dec_len}_neuron_decoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, max_dec_len], dtype=torch.int64), torch.zeros([1, 1500, dim_dec], dtype=torch.float32))\n",
    "    if hasattr(model.model.decoder, 'forward_neuron'): del model.model.decoder.forward_neuron\n",
    "    neuron_decoder = torch_neuronx.trace(\n",
    "        model.model.decoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16',\n",
    "        compiler_workdir='./dec_dir',      \n",
    "        inline_weights_to_neff=True)\n",
    "    neuron_decoder.save(model_filename)\n",
    "    model.model.decoder.forward_neuron = neuron_decoder\n",
    "else:\n",
    "    model.model.decoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aca1ee-7419-4e20-abb4-d0d7ae5093b5",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6ad3-e788-474b-8b21-31feea10872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup inf2 model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2495509b-ef66-49a9-a665-15578b159fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed inf2: 0.08130168914794922\n",
      "Elapsed cpu: 1.1965036392211914\n",
      "Tokens inf2: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Tokens cpu: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Out inf2: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n",
      "Out cpu: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "y1 = model.generate(input_features)\n",
    "print(f\"Elapsed inf2: {time.time()-t}\")\n",
    "t=time.time()\n",
    "y2 = cpu_model.generate(input_features)\n",
    "print(f\"Elapsed cpu: {time.time()-t}\")\n",
    "print(f\"Tokens inf2: {y1}\")\n",
    "print(f\"Tokens cpu: {y2}\")\n",
    "t1 = processor.batch_decode(y1, skip_special_tokens=True)\n",
    "t2 = processor.batch_decode(y2, skip_special_tokens=True)\n",
    "print(f\"Out inf2: {t1}\")\n",
    "print(f\"Out cpu: {t2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af72c44-1104-4464-8ae1-c3021dfe68cd",
   "metadata": {},
   "source": [
    "## Pipeline Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7bf44-f05f-4f6c-ad0a-5af5d2c187a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.1590416431427002\n",
      "{'text': ' He', 'timestamp': (0.0, 0.72)}\n",
      "{'text': ' has', 'timestamp': (0.72, 1.0)}\n",
      "{'text': ' grave', 'timestamp': (1.0, 1.36)}\n",
      "{'text': ' doubts', 'timestamp': (1.36, 1.7)}\n",
      "{'text': ' whether', 'timestamp': (1.7, 2.06)}\n",
      "{'text': ' Sir', 'timestamp': (2.06, 2.48)}\n",
      "{'text': ' Frederick', 'timestamp': (2.48, 2.82)}\n",
      "{'text': \" Layton's\", 'timestamp': (2.82, 3.36)}\n",
      "{'text': ' work', 'timestamp': (3.36, 3.72)}\n",
      "{'text': ' is', 'timestamp': (3.72, 3.98)}\n",
      "{'text': ' really', 'timestamp': (3.98, 4.48)}\n",
      "{'text': ' Greek', 'timestamp': (4.48, 4.86)}\n",
      "{'text': ' after', 'timestamp': (4.86, 5.3)}\n",
      "{'text': ' all', 'timestamp': (5.3, 6.0)}\n",
      "{'text': ' and', 'timestamp': (6.0, 6.36)}\n",
      "{'text': ' can', 'timestamp': (6.36, 6.54)}\n",
      "{'text': ' discover', 'timestamp': (6.54, 6.98)}\n",
      "{'text': ' in', 'timestamp': (6.98, 7.18)}\n",
      "{'text': ' it', 'timestamp': (7.18, 7.38)}\n",
      "{'text': ' but', 'timestamp': (7.38, 7.6)}\n",
      "{'text': ' little', 'timestamp': (7.6, 7.96)}\n",
      "{'text': ' of', 'timestamp': (7.96, 8.16)}\n",
      "{'text': ' Rocky', 'timestamp': (8.16, 8.84)}\n",
      "{'text': ' Ithaca.', 'timestamp': (8.84, 9.86)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, WhisperProcessor\n",
    "\n",
    "if not output_attentions:\n",
    "    raise Exception(\"Word timestamp not supported. Please set output_attentions=True and recompile the model\")\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model_id,\n",
    "  chunk_length_s=30\n",
    ")\n",
    "pipe.model = model\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[3][\"audio\"]\n",
    "\n",
    "# we can also return timestamps for the predictions\n",
    "## Option return_timestamps can be: True, False, \"word\" or \"char\"\n",
    "t=time.time()\n",
    "prediction = pipe(sample.copy(), batch_size=1, return_timestamps=\"word\")[\"chunks\"]\n",
    "print(f\"Elapsed: {time.time()-t}\")\n",
    "for p in prediction:\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
