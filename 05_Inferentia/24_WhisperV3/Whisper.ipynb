{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd387845-cc83-4b60-8ca0-53c26fba0531",
   "metadata": {},
   "source": [
    "# Whisper for Inferentia2\n",
    "\n",
    "**Instance**: EC2 ml.inf2.24xlarge  \n",
    "**Deep Learning Container**: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.18.1-ubuntu20.04\n",
    "\n",
    "### Requirements\n",
    " - transformers==4.36.2\n",
    " - soundfile==0.12.1\n",
    " - datasets==2.18.0\n",
    " - librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5448bf-dec2-493c-97f2-9acff99ff7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers==4.36.2 datasets==2.18.0 soundfile==0.12.1 librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2beede5-4669-4aa7-a0bd-98d81823a711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "import torch\n",
    "import neuronx_distributed\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "suffix=\"tiny\"\n",
    "#suffix=\"small\"\n",
    "#suffix=\"medium\"\n",
    "#suffix=\"large-v3\"\n",
    "model_id=f\"openai/whisper-{suffix}\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "cpu_model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "\n",
    "# Load a sample\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = dataset[3][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "batch_size=1\n",
    "max_dec_len = 64\n",
    "# num_mel_bins,d_model\n",
    "if suffix in \"tiny\":\n",
    "    dim_enc,dim_dec=80,384\n",
    "elif suffix in \"small\":\n",
    "    dim_enc,dim_dec=80,768\n",
    "elif suffix in \"medium\":\n",
    "    dim_enc,dim_dec=80,1024\n",
    "elif suffix == \"large-v3\":\n",
    "    dim_enc,dim_dec=128,1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27073a8f-6efd-49a6-a857-386762add8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions,BaseModelOutput\n",
    "\n",
    "def enc_f(self, input_features, attention_mask, **kwargs):\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(input_features, attention_mask)['last_hidden_state']\n",
    "    else:\n",
    "        out = self.forward_(input_features, attention_mask, return_dict=False)[0]\n",
    "    return BaseModelOutput(last_hidden_state=out)\n",
    "\n",
    "def dec_f(self, input_ids, attention_mask=None, encoder_hidden_states=None, **kwargs):\n",
    "    out = None        \n",
    "    if not attention_mask is None and encoder_hidden_states is None:\n",
    "        ## I know, this is a quick-n-dirty workaround to align the input parameters for NeuronSDK tracer\n",
    "        encoder_hidden_states, attention_mask = attention_mask,encoder_hidden_states\n",
    "    inp = [input_ids, encoder_hidden_states]\n",
    "    # pad the input to max_dec_len\n",
    "    if inp[0].shape[1] > self.max_length:\n",
    "        raise Exception(f\"The decoded sequence is not supported. Max: {self.max_length}\")\n",
    "    pad_size = torch.as_tensor(self.max_length - inp[0].shape[1])\n",
    "    inp[0] = F.pad(inp[0], (0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(*inp)['last_hidden_state']\n",
    "    else:        \n",
    "        out = self.forward_(input_ids=inp[0], encoder_hidden_states=inp[1], return_dict=False, use_cache=False)[0]        \n",
    "    # unpad the output\n",
    "    last_hidden_state = out[:, :input_ids.shape[1], :]\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "    \n",
    "\n",
    "if not hasattr(model.model.encoder, 'forward_'): model.model.encoder.forward_ = model.model.encoder.forward\n",
    "if not hasattr(model.model.decoder, 'forward_'): model.model.decoder.forward_ = model.model.decoder.forward\n",
    "\n",
    "model.model.encoder.forward = types.MethodType(enc_f, model.model.encoder)\n",
    "model.model.decoder.forward = types.MethodType(dec_f, model.model.decoder)\n",
    "\n",
    "model.model.decoder.max_length = max_dec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd76350-916d-4747-8482-89636940ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346a219-4e02-40f9-8a57-a7c8752aab30",
   "metadata": {},
   "source": [
    "## Trace Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ec9a6-effd-4fae-acf4-1ce1a4109ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_neuron_encoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, dim_enc, 3000], dtype=torch.float32), torch.zeros([1, dim_enc], dtype=torch.int64))\n",
    "    if hasattr(model.model.encoder, 'forward_neuron'): del model.model.encoder.forward_neuron\n",
    "    neuron_encoder = torch_neuronx.trace(\n",
    "        model.model.encoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --enable-saturate-infinity --auto-cast=all', \n",
    "        compiler_workdir='./enc_dir',      \n",
    "        inline_weights_to_neff=False)\n",
    "    neuron_encoder.save(model_filename)\n",
    "    model.model.encoder.forward_neuron = neuron_encoder\n",
    "else:\n",
    "    model.model.encoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a8e74-7971-4f8e-b60a-84aed9bdd186",
   "metadata": {},
   "source": [
    "## Trace decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea4f30-f1a2-47f6-9583-1eb7d26ae882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_neuron_decoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, max_dec_len], dtype=torch.int64), torch.zeros([1, 1500, dim_dec], dtype=torch.float32))\n",
    "    if hasattr(model.model.decoder, 'forward_neuron'): del model.model.decoder.forward_neuron\n",
    "    neuron_decoder = torch_neuronx.trace(\n",
    "        model.model.decoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --enable-saturate-infinity  --auto-cast=all',\n",
    "        compiler_workdir='./dec_dir',      \n",
    "        inline_weights_to_neff=True)\n",
    "    neuron_decoder.save(model_filename)\n",
    "    model.model.decoder.forward_neuron = neuron_decoder\n",
    "else:\n",
    "    model.model.decoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aca1ee-7419-4e20-abb4-d0d7ae5093b5",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6ad3-e788-474b-8b21-31feea10872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup inf2 model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2495509b-ef66-49a9-a665-15578b159fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed inf2: 0.06917572021484375\n",
      "Elapsed cpu: 1.6093239784240723\n",
      "Tokens inf2: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Tokens cpu: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Out inf2: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n",
      "Out cpu: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "y1 = model.generate(input_features)\n",
    "print(f\"Elapsed inf2: {time.time()-t}\")\n",
    "t=time.time()\n",
    "y2 = cpu_model.generate(input_features)\n",
    "print(f\"Elapsed cpu: {time.time()-t}\")\n",
    "print(f\"Tokens inf2: {y1}\")\n",
    "print(f\"Tokens cpu: {y2}\")\n",
    "t1 = processor.batch_decode(y1, skip_special_tokens=True)\n",
    "t2 = processor.batch_decode(y2, skip_special_tokens=True)\n",
    "print(f\"Out inf2: {t1}\")\n",
    "print(f\"Out cpu: {t2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af72c44-1104-4464-8ae1-c3021dfe68cd",
   "metadata": {},
   "source": [
    "## Pipeline Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e7bf44-f05f-4f6c-ad0a-5af5d2c187a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': (0.0, 6.4), 'text': \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can\"}\n",
      "{'timestamp': (6.4, 9.4), 'text': ' discover in it but little of Rocky Ithaca.'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, WhisperProcessor\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model_id,\n",
    "  chunk_length_s=30,\n",
    ")\n",
    "pipe.model = model\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[3][\"audio\"]\n",
    "\n",
    "# we can also return timestamps for the predictions\n",
    "prediction = pipe(sample.copy(), batch_size=1, return_timestamps=True)[\"chunks\"]\n",
    "for p in prediction:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135bb53-d519-42d9-b591-b48cbce8c159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
