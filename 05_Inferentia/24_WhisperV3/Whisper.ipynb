{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd387845-cc83-4b60-8ca0-53c26fba0531",
   "metadata": {},
   "source": [
    "# Whisper for Inferentia2\n",
    "\n",
    "This sample shows how to compile & run Whisper models (different sizes) on Inferentia2. It makes use of the HF weights:  \n",
    "  - Tiny: https://huggingface.co/openai/whisper-tiny\n",
    "  - Small: https://huggingface.co/openai/whisper-small\n",
    "  - Medium: https://huggingface.co/openai/whisper-medium\n",
    "  - Large-v3: https://huggingface.co/openai/whisper-large-v3\n",
    "\n",
    "Given the largest model has only 1.5B params, it fits into just 1 core when quantized to bf16. Also, this model is an encoder-decoder, so the strategy is to compile both components individually and then put them back into the original model structure. After that, both encoder and decoder will be accelerated on inf2.\n",
    "\n",
    "You can use the smallest instance for this experiment: inf2.xlarge, but to achieve a higher througput by launching multiple copies of the model to serve clients in parallel, it is recommended to use a larger instance like:\n",
    "\n",
    "**Instance**: EC2 ml.inf2.24xlarge  \n",
    "\n",
    "Follow the [instructions from this page to setup the environment.](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx) I also recommend the usage of the following container (DLC) to run your experiments: **Deep Learning Container**: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.18.1-ubuntu20.04\n",
    "\n",
    "This will guarantee you're using the exact same libraries I used in this experiment.\n",
    "\n",
    "Also, make sure you install the following libraries in your environment. Pay attention to the transformers version, newer versions will not work.\n",
    "\n",
    "### Requirements\n",
    " - transformers==4.36.2\n",
    " - soundfile==0.12.1\n",
    " - datasets==2.18.0\n",
    " - librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5448bf-dec2-493c-97f2-9acff99ff7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers==4.36.2 datasets==2.18.0 soundfile==0.12.1 librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2beede5-4669-4aa7-a0bd-98d81823a711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "# please, start by selecting the desired model size\n",
    "suffix=\"tiny\"\n",
    "#suffix=\"small\"\n",
    "#suffix=\"medium\"\n",
    "#suffix=\"large-v3\"\n",
    "model_id=f\"openai/whisper-{suffix}\"\n",
    "\n",
    "# this will load the tokenizer + two copies of the model. cpu_model will be used later for results comparison\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "cpu_model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "\n",
    "# Load a sample from the dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# sample #3 is ~9.9seconds and produces 33 output tokens + pad token\n",
    "sample = dataset[3][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "batch_size=1\n",
    "# this is the maximum number of tokens the model will be able to decode\n",
    "# for the sample #3 we selected above, this is enough. If you're planning to \n",
    "# process larger samples, you need to adjust it accordinly.\n",
    "max_dec_len = 64\n",
    "# num_mel_bins,d_model --> these parameters where copied from model.conf (found on HF repo)\n",
    "# we need them to correctly generate dummy inputs during compilation\n",
    "if suffix in \"tiny\":\n",
    "    dim_enc,dim_dec=80,384\n",
    "elif suffix in \"small\":\n",
    "    dim_enc,dim_dec=80,768\n",
    "elif suffix in \"medium\":\n",
    "    dim_enc,dim_dec=80,1024\n",
    "elif suffix == \"large-v3\":\n",
    "    dim_enc,dim_dec=128,1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27073a8f-6efd-49a6-a857-386762add8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions,BaseModelOutput\n",
    "\n",
    "# Now we need to simplify both encoding & decoding forward methods to make them \n",
    "# compilable. Please notice that these methods overwrite the original ones, but\n",
    "# keeps retro-compatibility. Also, we'll use use a new variable \"forward_neuron\"\n",
    "# to invoke the model on inf2\n",
    "def enc_f(self, input_features, attention_mask, **kwargs):\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(input_features, attention_mask)['last_hidden_state']\n",
    "    else:\n",
    "        out = self.forward_(input_features, attention_mask, return_dict=False)[0]\n",
    "    return BaseModelOutput(last_hidden_state=out)\n",
    "\n",
    "def dec_f(self, input_ids, attention_mask=None, encoder_hidden_states=None, **kwargs):\n",
    "    out = None        \n",
    "    if not attention_mask is None and encoder_hidden_states is None:\n",
    "        # this is a workaround to align the input parameters for NeuronSDK tracer\n",
    "        # None values are not allowed during compilation\n",
    "        encoder_hidden_states, attention_mask = attention_mask,encoder_hidden_states\n",
    "    inp = [input_ids, encoder_hidden_states]\n",
    "    \n",
    "    # here we pad the input to max_dec_len\n",
    "    if inp[0].shape[1] > self.max_length:\n",
    "        raise Exception(f\"The decoded sequence is not supported. Max: {self.max_length}\")\n",
    "    pad_size = torch.as_tensor(self.max_length - inp[0].shape[1])\n",
    "    inp[0] = F.pad(inp[0], (0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(*inp)['last_hidden_state']\n",
    "    else:        \n",
    "        out = self.forward_(input_ids=inp[0], encoder_hidden_states=inp[1], return_dict=False, use_cache=False)[0]        \n",
    "    # unpad the output\n",
    "    last_hidden_state = out[:, :input_ids.shape[1], :]\n",
    "    \n",
    "    return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "    \n",
    "# save a backup of the original forward method\n",
    "if not hasattr(model.model.encoder, 'forward_'): model.model.encoder.forward_ = model.model.encoder.forward\n",
    "if not hasattr(model.model.decoder, 'forward_'): model.model.decoder.forward_ = model.model.decoder.forward\n",
    "\n",
    "# overwrite with the new methods\n",
    "model.model.encoder.forward = types.MethodType(enc_f, model.model.encoder)\n",
    "model.model.decoder.forward = types.MethodType(dec_f, model.model.decoder)\n",
    "\n",
    "model.model.decoder.max_length = max_dec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd76350-916d-4747-8482-89636940ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346a219-4e02-40f9-8a57-a7c8752aab30",
   "metadata": {},
   "source": [
    "## Trace Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ec9a6-effd-4fae-acf4-1ce1a4109ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_neuron_encoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, dim_enc, 3000], dtype=torch.float32), torch.zeros([1, dim_enc], dtype=torch.int64))\n",
    "    if hasattr(model.model.encoder, 'forward_neuron'): del model.model.encoder.forward_neuron\n",
    "    neuron_encoder = torch_neuronx.trace(\n",
    "        model.model.encoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --enable-saturate-infinity --auto-cast=all', \n",
    "        compiler_workdir='./enc_dir',      \n",
    "        inline_weights_to_neff=False)\n",
    "    neuron_encoder.save(model_filename)\n",
    "    model.model.encoder.forward_neuron = neuron_encoder\n",
    "else:\n",
    "    model.model.encoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a8e74-7971-4f8e-b60a-84aed9bdd186",
   "metadata": {},
   "source": [
    "## Trace decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea4f30-f1a2-47f6-9583-1eb7d26ae882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "model_filename=f\"whisper_{suffix}_{batch_size}_neuron_decoder.pt\"\n",
    "if not os.path.isfile(model_filename):\n",
    "    inp = (torch.zeros([1, max_dec_len], dtype=torch.int64), torch.zeros([1, 1500, dim_dec], dtype=torch.float32))\n",
    "    if hasattr(model.model.decoder, 'forward_neuron'): del model.model.decoder.forward_neuron\n",
    "    neuron_decoder = torch_neuronx.trace(\n",
    "        model.model.decoder, \n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --enable-saturate-infinity  --auto-cast=all',\n",
    "        compiler_workdir='./dec_dir',      \n",
    "        inline_weights_to_neff=True)\n",
    "    neuron_decoder.save(model_filename)\n",
    "    model.model.decoder.forward_neuron = neuron_decoder\n",
    "else:\n",
    "    model.model.decoder.forward_neuron = torch.jit.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aca1ee-7419-4e20-abb4-d0d7ae5093b5",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6ad3-e788-474b-8b21-31feea10872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup inf2 model\n",
    "y1 = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2495509b-ef66-49a9-a665-15578b159fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed inf2: 0.06917572021484375\n",
      "Elapsed cpu: 1.6093239784240723\n",
      "Tokens inf2: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Tokens cpu: tensor([[50258, 50259, 50359, 50363,   634,   575, 12525, 22618,  1968,  6144,\n",
      "         35617, 20084,  1756,   311,   589,   307,   534, 10281,   934,   439,\n",
      "           293,   393,  4411,   294,   309,   457,   707,   295, 26916,   286,\n",
      "           392,  6628,    13, 50257]])\n",
      "Out inf2: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n",
      "Out cpu: [\" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "y1 = model.generate(input_features)\n",
    "print(f\"Elapsed inf2: {time.time()-t}\")\n",
    "t=time.time()\n",
    "y2 = cpu_model.generate(input_features)\n",
    "print(f\"Elapsed cpu: {time.time()-t}\")\n",
    "print(f\"Tokens inf2: {y1}\")\n",
    "print(f\"Tokens cpu: {y2}\")\n",
    "t1 = processor.batch_decode(y1, skip_special_tokens=True)\n",
    "t2 = processor.batch_decode(y2, skip_special_tokens=True)\n",
    "print(f\"Out inf2: {t1}\")\n",
    "print(f\"Out cpu: {t2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af72c44-1104-4464-8ae1-c3021dfe68cd",
   "metadata": {},
   "source": [
    "## Pipeline Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e7bf44-f05f-4f6c-ad0a-5af5d2c187a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': (0.0, 6.4), 'text': \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can\"}\n",
      "{'timestamp': (6.4, 9.4), 'text': ' discover in it but little of Rocky Ithaca.'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, WhisperProcessor\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model_id,\n",
    "  chunk_length_s=30,\n",
    ")\n",
    "pipe.model = model\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[3][\"audio\"]\n",
    "\n",
    "# we can also return timestamps for the predictions\n",
    "prediction = pipe(sample.copy(), batch_size=1, return_timestamps=True)[\"chunks\"]\n",
    "for p in prediction:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135bb53-d519-42d9-b591-b48cbce8c159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
