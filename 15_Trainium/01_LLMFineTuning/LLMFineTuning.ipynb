{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b4985d-8a27-4570-b742-2632bc843fc6",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL\n",
    "## Finetune LLMs using multi-host SageMaker distributed Training with Trainium and HF Optimum Neuron\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "In this sample we'll use HF Optimum Neuron to fine-tune LLama2 or Mistral using Trainium/SageMaker Distributed training. For that we need to make sure we correctly configure the code, hyperparameters, envvars and other things.\n",
    "\n",
    "**MAKE SURE**\n",
    " 1) To use HF Optimum Neuron version **0.0.19** or later. Previous versions have a bug in the **torchrun** initialization that causes training failures --> timeout showing issues with **127.0.0.1 5000**.\n",
    " 2) To split your dataset correctly and use at minimum **FastFile mode** to minimize the IO Overhead\n",
    " 3) To use a memory efficient mechanism, like Pyarrow, to load the chunks of the dataset into memory to avoid OOM issues\n",
    " 4) To correctly set the envvar **MALLOC_ARENA_MAX** to avoid OOM issues\n",
    " 5) (Optional) To run a complete training job with 1 host only to create the cache files, before running the full training\n",
    " 6) **NEVER** use **ShardedByS3Key** to distribute the samples of your dataset.\n",
    "\n",
    "### Other Tips\n",
    "\n",
    "**1) If you see the following WARN: 2024-Feb-28 11:37:42.0289 452:647 [0] include/socket.h:541 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying**, check: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/training-troubleshooting.html#nccl-warning-nccl-warn-timeout-waiting-for-rx-waited-120-sec-retrying\n",
    "\n",
    "**2) \"file not found\" issue when compilig cache files**, check https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/training-troubleshooting.html?highlight=CACHE_DIR#compilation-errors-when-placing-neuroncache-home-directory-on-nfs-efs-fsx-mounted-drive\n",
    "\n",
    "**3)  ERROR  TDRV:v2_cc_execute                           [nec_dev 22, gid 22] MPMD detected but reload is not supported yet:** if each node receives different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c062b-5161-42b8-a597-9bf39ed6d274",
   "metadata": {},
   "source": [
    "## 01) Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733adde-9bfa-49fa-8d57-10c765efc2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pyarrow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4ab1c-76b0-4c3c-8180-28c865072348",
   "metadata": {},
   "source": [
    "## 02) Download, split and upload the dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b6a7-de18-4809-897c-f82e7a7ee2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region_name='us-east-1'\n",
    "boto_session = boto3.Session(region_name=region_name)\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3013eb9-a71d-4826-8cf4-6c5ee6b03293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 32\n",
    "max_rows_per_file = batch_size * 32\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "schema = pa.schema([('text', pa.string())])\n",
    "\n",
    "for name in ['train', 'test']:\n",
    "    os.makedirs(f\"dataset/{name}\", exist_ok=True)\n",
    "    num_rows = len(dataset.data[name])\n",
    "    print(f\"Num unfiltered rows for {name}: {num_rows}\")\n",
    "    \n",
    "    rows = []\n",
    "    sink = None\n",
    "    file_id = 0\n",
    "    writer = None    \n",
    "    num_good_rows_per_file = 0\n",
    "    \n",
    "    for row_id, row in enumerate(dataset[name]):\n",
    "        text = row['text'].strip()\n",
    "        eof = row_id == num_rows-1\n",
    "        \n",
    "        if writer is None:\n",
    "            # start a new writer + file if required\n",
    "            sink = io.BytesIO()\n",
    "            writer = pa.ipc.new_file(sink, schema)\n",
    "            num_good_rows_per_file = 0\n",
    "            \n",
    "        if len(text) > 0:\n",
    "            # ignore empty rows, but continue the flow anyway\n",
    "            rows.append(text)\n",
    "            num_good_rows_per_file += 1      \n",
    "            \n",
    "        if eof or len(rows) == batch_size:\n",
    "            # Ok. We have enough rows for the batch or we reached the end of rows\n",
    "            batch = pa.record_batch([rows], schema)\n",
    "            writer.write(batch)\n",
    "            rows = []\n",
    "            \n",
    "        if eof or num_good_rows_per_file == max_rows_per_file:\n",
    "            # write a new file with the batches we collected so far\n",
    "            filename = f\"{file_id:010d}.arrow\"\n",
    "            writer.close()\n",
    "            \n",
    "            print(f\"Trying to write file {filename}, rows: {num_good_rows_per_file}\")\n",
    "            #sink.seek(0)\n",
    "            #with open(f\"dataset/{name}/{filename}\", \"wb\") as f: f.write(sink.read())\n",
    "            sink.seek(0)\n",
    "            s3_uri = sess.upload_string_as_file_body(sink.read(), bucket=bucket, key=f'datasets/wikitest/{name}/{filename}')\n",
    "            print(s3_uri)      \n",
    "                        \n",
    "            sink.close()\n",
    "            \n",
    "            writer = None\n",
    "            sink = None\n",
    "            file_id += 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545653-3174-4c1c-bdf8-ab28ad02d392",
   "metadata": {},
   "source": [
    "## 02) Training script + requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4ec47-5c87-40fd-947b-78cae5115806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"src\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd009e2-2ad2-4e2d-a9ef-f617f943ce8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "import os\n",
    "import glob\n",
    "import socket\n",
    "import argparse\n",
    "import transformers\n",
    "import pyarrow as pa\n",
    "\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from optimum.neuron import NeuronTrainer as Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.neuron.distributed import lazy_load_for_parallelism\n",
    "from optimum.neuron import NeuronTrainingArguments as TrainingArguments\n",
    "\n",
    "#from transformers.models.mistral.modeling_mistral import MistralDecoderLayer\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--max_sen_len\", type=int, default=512)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4)    \n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--tp_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--pp_size\", type=int, default=1)\n",
    "        \n",
    "    parser.add_argument(\"--model_id\", type=str, required=True)\n",
    "    parser.add_argument(\"--zero_1\", type=bool, default=True)\n",
    "    parser.add_argument(\"--task\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--collator_class\", type=str, default=\"DefaultDataCollator\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--bf16\", type=bool, default=True)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"output\"))\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"model\"))\n",
    "    \n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "    parser.add_argument(\"--eval_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_EVAL\", None))\n",
    "\n",
    "    parser.add_argument(\"--hf_cache_repo\", type=str, default=None)\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    # workaround for \"file not found\" issue: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/training-troubleshooting.html?highlight=CACHE_DIR#compilation-errors-when-placing-neuroncache-home-directory-on-nfs-efs-fsx-mounted-drive\n",
    "    #os.environ['NEURON_CC_FLAGS'] = f'--cache_dir={os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"output\")}/.neuron_cache/{socket.gethostname()}'\n",
    "\n",
    "    if not args.hf_token is None:\n",
    "        login(args.hf_token)\n",
    "    \n",
    "    class ArrowStreamDataset(Dataset):\n",
    "        '''Dataset that streams batches instead of loading the whole file into memory'''\n",
    "        def __init__(self, file_dir, batch_size=32, max_rows_per_file=32 * 32):\n",
    "            self.batch_size = batch_size\n",
    "            self.max_rows_per_file = max_rows_per_file\n",
    "\n",
    "            data_files = sorted(glob.glob(os.path.join(file_dir, \"*.arrow\")))\n",
    "            source = [pa.memory_map(f, 'rb') for f in data_files]\n",
    "            self.data = [pa.ipc.open_file(s) for s in source]\n",
    "            self.num_batches = [d.num_record_batches for d in self.data]\n",
    "\n",
    "            self.rows_last_batch = [d.get_batch(self.num_batches[i]-1).num_rows for i,d in enumerate(self.data)]\n",
    "            self.num_rows = 0\n",
    "            for b,r in zip(self.num_batches, self.rows_last_batch):\n",
    "                self.num_rows += (b*batch_size) - (batch_size-r)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_rows\n",
    "\n",
    "        def __getitem__(self, idx):        \n",
    "            file_id = idx // self.max_rows_per_file\n",
    "            i_id = idx % self.max_rows_per_file\n",
    "\n",
    "            batch_id = i_id // self.batch_size\n",
    "            row_id = i_id % self.batch_size\n",
    "\n",
    "            batch = self.data[file_id].get_batch(batch_id)\n",
    "            item = batch.take([row_id]).to_pydict()['text'][0]\n",
    "            return item\n",
    "    \n",
    "    # load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, token=args.hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.model_max_length = args.max_sen_len\n",
    "\n",
    "    # Custom collator to add labels in the input sample\n",
    "    def data_collator(examples):\n",
    "        global tokenizer\n",
    "        inputs = tokenizer(examples, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        inputs['labels'] = inputs.input_ids\n",
    "        return inputs\n",
    "\n",
    "    # Instantiate the datasets\n",
    "    train_dataset=ArrowStreamDataset(args.training_dir)\n",
    "    eval_dataset=ArrowStreamDataset(args.eval_dir)\n",
    "    \n",
    "    # load the model using the lazy paralellizer\n",
    "    with lazy_load_for_parallelism(tensor_parallel_size=args.tp_size):\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.model_id, token=args.hf_token, low_cpu_mem_usage=True)\n",
    "    \n",
    "    # Specify the `tensor_parallel_size` in the training arguments.\n",
    "    training_args = TrainingArguments(\n",
    "        zero_1=args.zero_1,\n",
    "        bf16=args.bf16,\n",
    "        tensor_parallel_size=args.tp_size,\n",
    "        #pipeline_parallel_size=pp_size,\n",
    "        #pipeline_config={\n",
    "        #    \"transformer_layer_cls\": MistralDecoderLayer, \n",
    "        #    \"num_microbatches\": pp_size + 2\n",
    "        #},\n",
    "        #sequence_parallel_enabled=True,\n",
    "        disable_embedding_parallelization=False, # It is `False` by default.\n",
    "        disable_sequence_parallel=False, # It is `False` by default.\n",
    "    \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        \n",
    "        num_train_epochs=args.epochs,\n",
    "        output_dir=args.output_data_dir,\n",
    "        overwrite_output_dir=True,        \n",
    "    \n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        \n",
    "        gradient_accumulation_steps=1,\n",
    "        eval_accumulation_steps=1,\n",
    "        \n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        hub_token=args.hf_token,\n",
    "        hub_model_id=args.hf_cache_repo\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b73b1-2339-4250-96c8-8308c412f5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "evaluate==0.4.1\n",
    "accelerate==0.23.0\n",
    "scikit-learn==1.4.0\n",
    "transformers==4.36.2\n",
    "optimum-neuron==0.0.19\n",
    "neuronx-distributed==0.6.0\n",
    "transformers-neuronx==0.9.474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37140896-9cd4-4dbd-8a34-da0dac955230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable\n",
    "HF_TOKEN=None\n",
    "\n",
    "assert not HF_TOKEN is None, \"Go to your HF account and get an access token. Set HF_TOKEN to your token\"\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d6789-3b54-49cd-852c-9527a2b3c834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "batch_size=4\n",
    "max_seq_len=512\n",
    "#model_id=\"mistralai/Mistral-7B-v0.1\"\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "CUSTOM_CACHE_REPO=\"samir-souza/llama-2-7b-hf\"\n",
    "instance_type='ml.trn1.32xlarge'\n",
    "\n",
    "print(f\"Instance type: {instance_type}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=2,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    input_mode='FastFile',\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.17.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    environment={\n",
    "        # Uncomment the following line to precompile the cache files\n",
    "        #\"RUN_NEURON_PARALLEL_COMPILE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"1\",\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",        \n",
    "        \"NEURON_RT_STOCHASTIC_ROUNDING_EN\": \"1\",\n",
    "        \"CUSTOM_CACHE_REPO\": CUSTOM_CACHE_REPO,\n",
    "        \"MALLOC_ARENA_MAX\":\"80\" # required to avoid OOM\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"zero_1\": True,\n",
    "        \"max_seq_len\": max_seq_len,\n",
    "        \"hf_token\": HF_TOKEN,\n",
    "        \"tp_size\": 8,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"model_id\": model_id\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e279f96-0f8e-412a-a7e8-746a42b62429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "estimator.fit({\n",
    "    # \"train\": f\"s3://{bucket}/datasets/wikitest/train\",\n",
    "    # \"eval\": f\"s3://{bucket}/datasets/wikitest/test\"\n",
    "    'train': TrainingInput(\n",
    "        f\"s3://{bucket}/datasets/wikitest/train\", distribution='FullyReplicated', compression='Gzip', input_mode='FastFile'\n",
    "    ),\n",
    "    'eval': TrainingInput(\n",
    "        f\"s3://{bucket}/datasets/wikitest/test\", distribution='FullyReplicated', compression='Gzip', input_mode='FastFile'\n",
    "    )\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
