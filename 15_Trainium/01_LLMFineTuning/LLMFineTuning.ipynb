{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b4985d-8a27-4570-b742-2632bc843fc6",
   "metadata": {},
   "source": [
    "# Fine-tune LLMs using multi-host SageMaker distributed Training with Trainium and HF Optimum Neuron\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "In this sample we'll use HF Optimum Neuron to fine-tune LLama2 or Mistral using Trainium/SageMaker Distributed training. For that we need to make sure we correctly configure the code, hyperparameters, envvars and other things.\n",
    "\n",
    "**MAKE SURE**\n",
    " 1) To split your dataset correctly and use at minimum **FastFile mode** to minimize the IO/Memory Overhead;\n",
    " 2) To use a memory efficient mechanism, like Pyarrow. Load chunks of the dataset into memory to avoid OOM issues;\n",
    " 3) To correctly set the envvar **MALLOC_ARENA_MAX** to avoid OOM issues;\n",
    " 6) **NEVER** use **ShardedByS3Key** to distribute the samples of your dataset.\n",
    "\n",
    "### Other Tips\n",
    "\n",
    "**1) If you see the following WARN: 2024-Feb-28 11:37:42.0289 452:647 [0] include/socket.h:541 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying**, check: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/training-troubleshooting.html#nccl-warning-nccl-warn-timeout-waiting-for-rx-waited-120-sec-retrying\n",
    "\n",
    "**2) \"file not found\" issue when compilig cache files**, check https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/training-troubleshooting.html?highlight=CACHE_DIR#compilation-errors-when-placing-neuroncache-home-directory-on-nfs-efs-fsx-mounted-drive\n",
    "\n",
    "**3)  ERROR  TDRV:v2_cc_execute                           [nec_dev 22, gid 22] MPMD detected but reload is not supported yet:** if each node receives different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c062b-5161-42b8-a597-9bf39ed6d274",
   "metadata": {},
   "source": [
    "## 01) Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733adde-9bfa-49fa-8d57-10c765efc2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pyarrow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4ab1c-76b0-4c3c-8180-28c865072348",
   "metadata": {},
   "source": [
    "## 02) Download, split and upload the dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b6a7-de18-4809-897c-f82e7a7ee2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3013eb9-a71d-4826-8cf4-6c5ee6b03293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 32\n",
    "max_rows_per_file = batch_size * 32\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "schema = pa.schema([('text', pa.string())])\n",
    "\n",
    "for name in ['train', 'test']:\n",
    "    os.makedirs(f\"dataset/{name}\", exist_ok=True)\n",
    "    num_rows = len(dataset.data[name])\n",
    "    print(f\"Num unfiltered rows for {name}: {num_rows}\")\n",
    "\n",
    "    rows = []\n",
    "    sink = None\n",
    "    file_id = 0\n",
    "    writer = None\n",
    "    num_good_rows_per_file = 0\n",
    "\n",
    "    for row_id, row in enumerate(dataset[name]):\n",
    "        text = row['text'].strip()\n",
    "        eof = row_id == num_rows-1\n",
    "        \n",
    "        if writer is None:\n",
    "            # start a new writer + file if required\n",
    "            sink = io.BytesIO()\n",
    "            writer = pa.ipc.new_file(sink, schema)\n",
    "            num_good_rows_per_file = 0\n",
    "        \n",
    "        if len(text) > 0:\n",
    "            # ignore empty rows, but continue the flow anyway\n",
    "            rows.append(text)\n",
    "            num_good_rows_per_file += 1\n",
    "        \n",
    "        if eof or len(rows) == batch_size:\n",
    "            # Ok. We have enough rows for the batch or we reached the end of rows\n",
    "            batch = pa.record_batch([rows], schema)\n",
    "            writer.write(batch)\n",
    "            rows = []\n",
    "        \n",
    "        if eof or num_good_rows_per_file == max_rows_per_file:\n",
    "            # write a new file with the batches we collected so far\n",
    "            filename = f\"{file_id:010d}.arrow\"\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Trying to write file {filename}, rows: {num_good_rows_per_file}\")\n",
    "            sink.seek(0)\n",
    "            s3_uri = sess.upload_string_as_file_body(sink.read(), bucket=bucket, key=f'datasets/wikitest/{name}/{filename}')\n",
    "            print(s3_uri)\n",
    "\n",
    "            sink.close()\n",
    "\n",
    "            writer = None\n",
    "            sink = None\n",
    "            file_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545653-3174-4c1c-bdf8-ab28ad02d392",
   "metadata": {},
   "source": [
    "## 02) Training script + requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4ec47-5c87-40fd-947b-78cae5115806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"src\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd009e2-2ad2-4e2d-a9ef-f617f943ce8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "import os\n",
    "import glob\n",
    "import socket\n",
    "import argparse\n",
    "import transformers\n",
    "import pyarrow as pa\n",
    "\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from optimum.neuron import NeuronTrainer as Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.neuron.distributed import lazy_load_for_parallelism\n",
    "from optimum.neuron import NeuronTrainingArguments as TrainingArguments\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--max_sen_len\", type=int, default=512)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4)    \n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--tp_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--pp_size\", type=int, default=1)\n",
    "        \n",
    "    parser.add_argument(\"--model_id\", type=str, required=True)\n",
    "    parser.add_argument(\"--zero_1\", type=bool, default=True)\n",
    "    parser.add_argument(\"--task\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--collator_class\", type=str, default=\"DefaultDataCollator\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--bf16\", type=bool, default=True)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"output\"))\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"model\"))\n",
    "    \n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "    parser.add_argument(\"--eval_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_EVAL\", None))\n",
    "    \n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        print(\"HF token defined. Logging in...\")\n",
    "        login(token=args.hf_token)\n",
    "\n",
    "    # Custom Data Loader that minimizes Memory Utilization while\n",
    "    # stream multiple big files from S3. It expects the files to be \n",
    "    # prepared the way you see in the initial sections of the original Notebook.\n",
    "    class ArrowStreamDataset(Dataset):\n",
    "        '''Dataset that streams batches instead of loading the whole file into memory'''\n",
    "        def __init__(self, file_dir, batch_size=32, max_rows_per_file=32 * 32):\n",
    "            self.batch_size = batch_size\n",
    "            self.max_rows_per_file = max_rows_per_file\n",
    "\n",
    "            data_files = sorted(glob.glob(os.path.join(file_dir, \"*.arrow\")))\n",
    "            source = [pa.memory_map(f, 'rb') for f in data_files]\n",
    "            self.data = [pa.ipc.open_file(s) for s in source]\n",
    "            self.num_batches = [d.num_record_batches for d in self.data]\n",
    "\n",
    "            self.rows_last_batch = [d.get_batch(self.num_batches[i]-1).num_rows for i,d in enumerate(self.data)]\n",
    "            self.num_rows = 0\n",
    "            for b,r in zip(self.num_batches, self.rows_last_batch):\n",
    "                self.num_rows += (b*batch_size) - (batch_size-r)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_rows\n",
    "\n",
    "        def __getitem__(self, idx):        \n",
    "            file_id = idx // self.max_rows_per_file\n",
    "            i_id = idx % self.max_rows_per_file\n",
    "\n",
    "            batch_id = i_id // self.batch_size\n",
    "            row_id = i_id % self.batch_size\n",
    "\n",
    "            batch = self.data[file_id].get_batch(batch_id)\n",
    "            item = batch.take([row_id]).to_pydict()['text'][0]\n",
    "            return item\n",
    "\n",
    "    # load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, token=args.hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.model_max_length = args.max_sen_len\n",
    "\n",
    "    # Custom collator to add labels in the input sample\n",
    "    def data_collator(examples):\n",
    "        global tokenizer\n",
    "        inputs = tokenizer(examples, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        inputs['labels'] = inputs.input_ids\n",
    "        return inputs\n",
    "\n",
    "    # Instantiate the datasets\n",
    "    train_dataset=ArrowStreamDataset(args.training_dir)\n",
    "    eval_dataset=ArrowStreamDataset(args.eval_dir)\n",
    "        \n",
    "    # Specify the `tensor_parallel_size` in the training arguments.\n",
    "    training_args = TrainingArguments(\n",
    "        zero_1=args.zero_1,\n",
    "        bf16=args.bf16,\n",
    "        tensor_parallel_size=args.tp_size,        \n",
    "        pipeline_parallel_size=args.pp_size,\n",
    "    \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        \n",
    "        num_train_epochs=args.epochs,\n",
    "        output_dir=args.output_data_dir,\n",
    "        overwrite_output_dir=True,        \n",
    "    \n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        \n",
    "        gradient_accumulation_steps=1,\n",
    "        eval_accumulation_steps=1,\n",
    "        \n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        hub_token=args.hf_token\n",
    "    )\n",
    "    # load the model using the lazy paralellizer\n",
    "    with lazy_load_for_parallelism(tensor_parallel_size=args.tp_size, pipeline_parallel_size=args.pp_size):\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.model_id, token=args.hf_token, low_cpu_mem_usage=True)        \n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b73b1-2339-4250-96c8-8308c412f5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "optimum-neuron==0.0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37140896-9cd4-4dbd-8a34-da0dac955230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable\n",
    "HF_TOKEN=\"\"\n",
    "\n",
    "if HF_TOKEN == \"\": print(\" >>> Go to your HF account and get an access token. Set HF_TOKEN to your token if you want to define your own cache repo\")\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d6789-3b54-49cd-852c-9527a2b3c834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "tp_degree=8\n",
    "batch_size=4\n",
    "max_seq_len=512\n",
    "\n",
    "# Mistral consumes more host memory, that's why it is 70 vs 80 of llama2\n",
    "## Mistral and Llama2 are gated models, that's why we're using a fine-tuned Mistral here\n",
    "## To use gated models you need to provide a valid HF_TOKEN\n",
    "ARENA_MAX,model_id=70,\"yam-peleg/Experiment31-7B\"\n",
    "\n",
    "# ATTENTION: To use llama2 you need to pass HF_TOKEN of an account\n",
    "# with permission to download Llama2 weights, otherwise the training will fail\n",
    "#ARENA_MAX,model_id=128,\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# the default cache repo points to a public / read-only cache\n",
    "# You can point it to your own repo, but make sure you properly defined the HF token in the HF_TOKEN (above)\n",
    "CUSTOM_CACHE_REPO=\"aws-neuron/optimum-neuron-cache\"\n",
    "\n",
    "instance_type='ml.trn1.32xlarge'\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,\n",
    "    \"zero_1\": True,\n",
    "    \"bf16\": True,\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"tp_size\": tp_degree,\n",
    "    \"pp_size\": 1,\n",
    "    \"eval_batch_size\": batch_size,\n",
    "    \"train_batch_size\": batch_size,\n",
    "    \"model_id\": model_id\n",
    "}\n",
    "if not HF_TOKEN is None and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "\n",
    "print(f\"Instance type: {instance_type}\\nHyperparameters: {hyperparameters}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=2,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    input_mode='FastFile',\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.18.1-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    environment={\n",
    "        # Uncomment the following line to precompile the cache files\n",
    "        #\"RUN_NEURON_PARALLEL_COMPILE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"1\",\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"XLA_DOWNCAST_BF16\": \"1\",\n",
    "        \"NEURON_FUSE_SOFTMAX\": \"1\",\n",
    "        \"NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS\": \"5\",\n",
    "        \n",
    "        \"NEURON_RT_STOCHASTIC_ROUNDING_EN\": \"1\",\n",
    "        \"CUSTOM_CACHE_REPO\": CUSTOM_CACHE_REPO,\n",
    "        \"MALLOC_ARENA_MAX\": str(ARENA_MAX), # required to avoid OOM\n",
    "        \"NEURON_CC_FLAGS\": \"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity\"\n",
    "    },\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e279f96-0f8e-412a-a7e8-746a42b62429",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "estimator.fit({\n",
    "    'train': TrainingInput(\n",
    "        f\"s3://{bucket}/datasets/wikitest/train\", distribution='FullyReplicated', compression='Gzip', input_mode='FastFile'\n",
    "    ),\n",
    "    'eval': TrainingInput(\n",
    "        f\"s3://{bucket}/datasets/wikitest/test\", distribution='FullyReplicated', compression='Gzip', input_mode='FastFile'\n",
    "    )    \n",
    "})"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
