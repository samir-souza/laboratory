{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27330bc6",
   "metadata": {},
   "source": [
    "# Optimized input payload for CV & SageMaker Endpoints with TF\n",
    "\n",
    "In this example you'll see how to create a custom handler for a SageMaker endpoint that uses a TF2.x model for CV to process images. The compression in .jpeg is used to keep the payload as small as possible.\n",
    "\n",
    "This solution minimizes as much as possible the overhead of the traditional formats like: Json, CSV, protobuf, etc.  The [custom TF serving handler for SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html) also uses GRPC to communicate with Tensorflow server. It reduces even more the data transfer overhead. But it is important to keep in mind that even GRPC has a linear overhead when translating the predictions from proto/tensor format to numpy. If you need to improve this, you have to invoke the model directly in Tensorflow and remove Tensorflow Serving.\n",
    "\n",
    "If you're looking for a more elegant solution than just concatenating .jpeg files to create a stream, try [RecordIO](https://mesos.apache.org/documentation/latest/recordio/) or [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord). You can change the way you're encoding your payload and the way you're reading it inside the container before sending to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f55fc",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe23330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f'Default bucket: {bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2f1ca",
   "metadata": {},
   "source": [
    "## Download some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "img_urls = [\n",
    "    'https://images.unsplash.com/photo-1501386761578-eac5c94b800a',\n",
    "    'https://images.unsplash.com/photo-1543465077-db45d34b88a5',\n",
    "    'https://images.unsplash.com/photo-1552249007-6759fe2742b6'\n",
    "]\n",
    "for i,url in enumerate(img_urls):\n",
    "    urllib.request.urlretrieve(url, f\"image_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c86642",
   "metadata": {},
   "source": [
    "## Get a pre-trained model and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ca21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "with urllib.request.urlopen('https://spock.cloud/models/yolov3-keras.tar.gz') as m:\n",
    "    s3_uri = sagemaker_session.upload_string_as_file_body(m.read(), bucket, 'models/yolov3-keras/model.tar.gz')\n",
    "    print(s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4236ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('code'): os.mkdir('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "opencv-python\n",
    "tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b78b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import subprocess\n",
    "subprocess.call([\"chmod\", \"777\", \"/tmp\"])\n",
    "subprocess.check_call([\"apt\", \"update\", \"-y\"])\n",
    "subprocess.check_call([\"apt\", \"install\", \"-y\", \"libgl1\"]) # for libGL, required by opencv\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "max_batch_size=10\n",
    "max_image_side=512 + 5 # grpc overhead\n",
    "yolov3_output_size_per_image=6 * 1024 * 1024 # ~5.3MB of output per image\n",
    "\n",
    "MAX_GRPC_SEND_MESSAGE_LENGTH = max_batch_size * 3 * max_image_side * max_image_side * 4\n",
    "MAX_GRPC_RECEIVE_MESSAGE_LENGTH = max_batch_size * yolov3_output_size_per_image\n",
    "\n",
    "client = None\n",
    "def init_client(grpc_port): # this method initializes a single instance of the client\n",
    "    global client, request\n",
    "    if client is not None: return\n",
    "    options = [\n",
    "        ('grpc.max_send_message_length', MAX_GRPC_SEND_MESSAGE_LENGTH),\n",
    "        ('grpc.max_receive_message_length', MAX_GRPC_RECEIVE_MESSAGE_LENGTH)\n",
    "    ]\n",
    "    channel = grpc.insecure_channel(f'0.0.0.0:{grpc_port}', options=options)\n",
    "    client = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    \n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = 'yolov3'# this is the same name of the root dir in your model.tar.gz\n",
    "    request.model_spec.signature_name = 'serving_default'\n",
    "    print('Client initialized')\n",
    "\n",
    "def predict(payload):\n",
    "    global client, request\n",
    "    \n",
    "    request.inputs['input_1'].Clear()\n",
    "    request.inputs['input_1'].CopyFrom(tf.make_tensor_proto(payload))\n",
    "    # invoke the model\n",
    "    prediction = client.Predict(request, 25)\n",
    "    # get the predictions\n",
    "    result = []\n",
    "    for o in [ 'conv2d_58', 'conv2d_66', 'conv2d_74' ]:\n",
    "        result.append( tf.make_ndarray(prediction.outputs[o]) )\n",
    "    \n",
    "    # ATTENTION: you need to post-process the network output first!\n",
    "    # reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo.py\n",
    "    \n",
    "    # this is just a quick-n-dirty way to return all raw values in NPY format\n",
    "    result = np.concatenate([i.flatten() for i in result])\n",
    "    return result\n",
    "\n",
    "# customizing the request handler\n",
    "# More info: https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html\n",
    "def handler(data, context):\n",
    "    if context.request_content_type != 'application/octet-stream':\n",
    "        raise ValueError(f'Unsupported content type: {context.request_content_type or \"unknown\"}')\n",
    "    if context.accept_header != 'application/x-npy':\n",
    "        raise ValueError(f'Unsupported accept type: {context.accept_header or \"unknown\"}')\n",
    "    \n",
    "    init_client(context.grpc_port)\n",
    "    \n",
    "    # iterate through all the images and create a batch\n",
    "    data_len = int.from_bytes(data.read(4), 'big')\n",
    "    payload = []\n",
    "    while data_len > 0:\n",
    "        imgread_start_time = time.time()\n",
    "        img = np.frombuffer(data.read(data_len), dtype=np.uint8)\n",
    "        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert from BGR to RGB\n",
    "        img = img.astype(np.float32) / 255. # normalize\n",
    "        img = np.expand_dims(img, axis=0) # put in NHWC format\n",
    "        payload.append(img)\n",
    "        data_len = int.from_bytes(data.read(4), 'big')\n",
    "    if len(payload) == 0: raise ValueError('Empty input data!')\n",
    "    \n",
    "    # invoke the model\n",
    "    result = predict(np.vstack(payload))\n",
    "    \n",
    "    # pickle the resulting tensor and return it as NPY\n",
    "    buffer = io.BytesIO()\n",
    "    pickle.dump(result, buffer)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    return buffer.read(), context.accept_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a171084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "# Create a SageMaker model from the serialized model in S3\n",
    "model = TensorFlowModel(\n",
    "    model_data=s3_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session, # comment to make local mode work\n",
    "    framework_version=\"2.4\",\n",
    "    source_dir='code',\n",
    "    entry_point='code/inference.py' # https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#id33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce346b2",
   "metadata": {},
   "source": [
    "## Build a big binary stream with all your .jpeg files\n",
    "This is one of the most optimized ways to send multiple images to a remote webservice.\n",
    "It is not the most elegant way... There are specific formats for this operation like RecordIO or TFRecord, but both introduce overheads. If the overhead is not a problem for you, I recommend you to use one of these open formats to serialize data. However, if you're looking for something that minimizes the overhead at any cost, this can be a solution, but remember that you need to control the byte streaming on both sides (client and server)\n",
    "\n",
    "What we're going to do now is:\n",
    "- to read all images\n",
    "- make them square by padding with 0's\n",
    "- resize them to the shape expected by the Neural Network\n",
    "- compress the image back to .jpg\n",
    "- serialize all the images, intercalating the # of bytes each image has.\n",
    "\n",
    "To read this, you start by reading 4bytes (int32) to get the number of bytes of the 1st image, then you read the # of bytes you found from the stream and build your first .jpg file. Repeat this process for all images in the streaming until you get an EOF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b52692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer length: 1031488 bytes\n",
      "CPU times: user 3.78 s, sys: 945 ms, total: 4.73 s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "batch_size=10\n",
    "img_size = 512\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]\n",
    "buffer = io.BytesIO()\n",
    "for i in range(batch_size):\n",
    "    i = i%3\n",
    "    img = cv2.imread(f'image_{i}.jpg')\n",
    "    h,w,c = img.shape\n",
    "    if h!=w: # make it square but keep aspect ratio\n",
    "        sqr_size = max(h,w)\n",
    "        sqr_img = np.zeros((sqr_size, sqr_size, c), dtype=np.uint8)\n",
    "        sqr_img[:h, :w],img = img,sqr_img\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    _,img = cv2.imencode('.jpeg', img, encode_param) # compress again to jpeg (minimize the payload)\n",
    "    buffer.write(len(img).to_bytes(4,'big'))\n",
    "    buffer.write(img.tobytes())\n",
    "print(f'Buffer length: {buffer.getbuffer().nbytes} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ff168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 257 ms, sys: 168 ms, total: 425 ms\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "\n",
    "predictor.serializer = IdentitySerializer()\n",
    "predictor.deserializer = NumpyDeserializer()\n",
    "buffer.seek(0)\n",
    "%time preds = predictor.predict(buffer.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34fc7a",
   "metadata": {},
   "source": [
    "### Rebuild the output just to validate the process\n",
    "The recommendation is to add a post-processing step inside your code (in the container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f56210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 16, 16, 255), (10, 32, 32, 255), (10, 64, 64, 255)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N=1 -> 0.03s (output=5.3MB)\n",
    "# N=5 -> 0.151s (output=26.14MB)\n",
    "# N=10 -> 0.335s (output=52.29MB)\n",
    "# (N, 16, 16, 255) -> N*255KB, (N, 32, 32, 255) --> N*1020KB, (N, 64, 64, 255) -> N*4080KB\n",
    "offset=0\n",
    "output=[]\n",
    "for i in [16,32,64]:\n",
    "    pivot = offset+batch_size*i*i*255\n",
    "    output.append(preds[offset:pivot].reshape((batch_size,i,i,255)))\n",
    "    offset=pivot\n",
    "[o.shape for o in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610c56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
