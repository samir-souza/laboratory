{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example shows how to use Model Monitor to manually kick-off monitoring jobs\n",
    "This is necessary when you don't have a SageMaker Endpoint, like when you use SageMaker Edge Manager + CaptureData.\n",
    "\n",
    "Model Monitor by default only supports tabular data: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\n",
    "\n",
    "However, you can bring your own container to Model Monitor: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html if you want to create a custom CV monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!curl https://spock.cloud/datasets/wind_turbine/dataset.csv.gz |gunzip > data/dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix='windturbine'\n",
    "\n",
    "endpoint_name = 'windturbine_no_endpoint'\n",
    "monitoring_schedule_name = 'monitoring-schedule-windturbine'\n",
    "variant_name = 'AllTraffic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['_', 'ts', 'freemem', 'rps', 'voltage',\n",
    "        'qw', 'qx', 'qy', 'qz', 'gx', 'gy', 'gz', 'ax', 'ay', 'az', \n",
    "        'gearboxtemp', 'ambtemp', 'humidity', 'pressure', 'gas' ]\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv', sep=',', names=cols, low_memory=False)\n",
    "df = df.drop(['_'], axis=1)\n",
    "df.ts = pd.to_timedelta(df.ts, unit='ms')\n",
    "df.drop(['ts', 'freemem'], axis=1).to_csv('data/full_dataset.csv', index=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Data Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "endpoint_monitor.suggest_baseline(\n",
    "    baseline_dataset='data/full_dataset.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri='s3://{}/{}/monitoring/data_quality/baseline'.format(bucket, prefix),\n",
    "    wait=True,\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Model Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "model_quality_monitor.suggest_baseline(\n",
    "    baseline_dataset='data/full_dataset.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri = ='s3://{}/{}/monitoring/model_quality/baseline'.format(bucket, prefix),\n",
    "    problem_type='Regression',\n",
    "    inference_attribute= \"prediction\", # The column in the dataset that contains predictions.\n",
    "    #probability_attribute= \"probability\", # The column in the dataset that contains probabilities.\n",
    "    ground_truth_attribute= \"label\", # The column in the dataset that contains ground truth labels.\n",
    "    wait=True,\n",
    "    logs=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating synthetic (json lines) logs (simulating Data Capture in a real endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "from uuid import uuid4\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "def generate_log_file(df, num_rows):\n",
    "    now = datetime.datetime.today()\n",
    "    suffix = now.strftime(\"%Y/%m/%d/%H/%M-%S-%f\")[:-3]\n",
    "    key = '%s/monitoring/%s/%s/%s-%s.jsonl' % (\n",
    "        prefix, endpoint_name, variant_name, suffix, uuid4() )\n",
    "    \n",
    "    data = io.BytesIO()\n",
    "    for i in range(num_rows):\n",
    "        idx = random.randint(0, len(df))\n",
    "        line = \",\".join(df.iloc[idx].values.astype(str))\n",
    "        row = {\n",
    "            \"captureData\": {\n",
    "                \"endpointInput\": {\n",
    "                  \"observedContentType\": \"text/csv\",\n",
    "                  \"mode\": \"INPUT\",\n",
    "                  \"data\": line,\n",
    "                  \"encoding\": \"CSV\"\n",
    "                },\n",
    "                \"endpointOutput\": {\n",
    "                  \"observedContentType\": \"text/csv; charset=utf-8\",\n",
    "                  \"mode\": \"OUTPUT\",\n",
    "                  \"data\": \"1,2,3,4,5\",\n",
    "                  \"encoding\": \"CSV\"\n",
    "                }\n",
    "            },\n",
    "            \"eventMetadata\": {\n",
    "                \"eventId\": str(uuid4()),\n",
    "                \"inferenceTime\": now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            },\n",
    "            \"eventVersion\": \"0\"\n",
    "        }\n",
    "        data.write((\"%s\\n\" % json.dumps(row)).encode('utf-8'))\n",
    "    data.seek(0)\n",
    "    \n",
    "    s3_client.upload_fileobj(data, bucket, key)\n",
    "generate_log_file(df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import boto3\n",
    "\n",
    "def process_monitoring_logs(endpoint_monitor):\n",
    "    sm = boto3.client('sagemaker')\n",
    "    now = datetime.datetime.today()\n",
    "    suffix = now.strftime(\"%Y/%m/%d/%H\")\n",
    "    start_time = datetime.datetime(now.year, now.month, now.day, now.hour)\n",
    "    end_time = start_time + datetime.timedelta(hours=1)\n",
    "\n",
    "    # get the monitoring metadata\n",
    "    base_desc = endpoint_monitor.describe_latest_baselining_job()    \n",
    "    baseline_path = base_desc['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "    logs_path = \"%s/%s/%s\" % (endpoint_name,variant_name,suffix)\n",
    "    \n",
    "    s3_output = {\n",
    "        \"S3Uri\": 's3://{}/{}/monitoring/{}'.format(bucket, prefix, logs_path),\n",
    "        \"LocalPath\": \"/opt/ml/processing/output\",\n",
    "        \"S3UploadMode\": \"Continuous\"\n",
    "    }\n",
    "    # values for the processing job input\n",
    "    values = [\n",
    "        [ 'input_1', 's3://{}/{}/monitoring/{}'.format(bucket, prefix, logs_path),\n",
    "            '/opt/ml/processing/input/endpoint/{}'.format(logs_path) ], \n",
    "        [ 'baseline', '%s/statistics.json' % baseline_path,\n",
    "            '/opt/ml/processing/baseline/stats'],\n",
    "        [ 'constraints', '%s/constraints.json' % baseline_path,\n",
    "            '/opt/ml/processing/baseline/constraints']\n",
    "    ]\n",
    "    job_params = {\n",
    "        'ProcessingJobName': 'model-monitoring-%s' % time.strftime(\"%Y%m%d%H%M%S\"),\n",
    "        'ProcessingInputs': [{\n",
    "            'InputName': o[0],\n",
    "            'S3Input': { \n",
    "                'S3Uri': o[1], 'LocalPath': o[2], 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', \n",
    "                'S3CompressionType': 'None', 'S3DataDistributionType': 'FullyReplicated'\n",
    "            }} for o in values],\n",
    "        'ProcessingOutputConfig': { 'Outputs': [ {'OutputName': 'result','S3Output': s3_output } ] },\n",
    "        'ProcessingResources': base_desc['ProcessingResources'],\n",
    "        'AppSpecification': base_desc['AppSpecification'],\n",
    "        'RoleArn': base_desc['RoleArn'],\n",
    "        'Environment': {\n",
    "            'baseline_constraints': '/opt/ml/processing/baseline/constraints/constraints.json',\n",
    "            'baseline_statistics': '/opt/ml/processing/baseline/stats/statistics.json',\n",
    "            'dataset_format': '{\"sagemakerCaptureJson\":{\"captureIndexNames\":[\"endpointInput\",\"endpointOutput\"]}}',\n",
    "            'dataset_source': '/opt/ml/processing/input/endpoint',      \n",
    "            'output_path': '/opt/ml/processing/output',\n",
    "            'publish_cloudwatch_metrics': 'Enabled',\n",
    "            'sagemaker_monitoring_schedule_name': monitoring_schedule_name,\n",
    "            'sagemaker_endpoint_name': endpoint_name,\n",
    "            'start_time': start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            'end_time': end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        }\n",
    "    }\n",
    "    print(job_params)\n",
    "    sm.create_processing_job(**job_params)\n",
    "    waiter = sm.get_waiter('processing_job_completed_or_stopped')\n",
    "    waiter.wait( ProcessingJobName=job_params['ProcessingJobName'], WaiterConfig={'Delay': 30,'MaxAttempts': 20} )\n",
    "    return job_params['ProcessingJobName'], s3_output['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The processing job takes something like 5mins to run\n",
    "job_name, s3_output = process_monitoring_logs(endpoint_monitor)\n",
    "tokens = s3_output.split('/', 3)\n",
    "df = pd.read_json(sagemaker_session.read_s3_file(tokens[2], '%s/constraint_violations.json' % tokens[3]))\n",
    "df = pd.json_normalize(df.violations)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
